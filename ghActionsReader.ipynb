{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import subprocess\n",
    "import re\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArqManipulation:\n",
    "    \"\"\"\n",
    "    A utility class for file operations and data manipulation.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod \n",
    "    def read_parquet_file(parquet_file_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Reads a Parquet file and returns a DataFrame.\n",
    "\n",
    "        :param parquet_file_name: Path to the Parquet file.\n",
    "        :return: DataFrame with file contents.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(parquet_file_name):\n",
    "                print(f\"File '{parquet_file_name}' does not exist.\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            return pd.read_parquet(parquet_file_name)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error reading Parquet file '{parquet_file_name}': {e}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def save_df_to_parquet(df: pd.DataFrame, parquet_file_name: str):\n",
    "        \"\"\"\n",
    "        Saves a DataFrame to a Parquet file.\n",
    "\n",
    "        :param df: Dataframe to save.\n",
    "        :param parquet_file_name: Parqueet saving path.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(parquet_file_name), exist_ok=True)\n",
    "            df.to_parquet(parquet_file_name)\n",
    "            print(f\"DataFrame successfully saved to {parquet_file_name}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error saving DataFrame to Parquet file '{parquet_file_name}': {e}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_ansi_escape(base_str: str) -> str:\n",
    "        \"\"\"\n",
    "        Removes ANSI escape values from a string.\n",
    "\n",
    "        :param base_str: Unformmated string.\n",
    "        :return: Cleaned string.\n",
    "        \"\"\"\n",
    "        return re.sub(r'\\x1B\\[[0-9;]*[A-Za-z]', '', base_str)\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_stdout_json(base_str: str) -> dict:\n",
    "        \"\"\"\n",
    "        Parses JSON output from GitHub CLI after cleaning ANSI escape sequences.\n",
    "\n",
    "        :param base_str: The raw output string from the GitHub CLI.\n",
    "        :return: Parsed JSON dictionary.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            cleaned = ArqManipulation.clean_ansi_escape(base_str)\n",
    "            str_output = ''.join(cleaned.splitlines())\n",
    "            return json.loads(str_output)\n",
    "        except json.JSONDecodeError as e:\n",
    "            raise e\n",
    "\n",
    "    @staticmethod\n",
    "    def json_to_df(parsed_json: dict) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Converts a JSON dictionary to a sorted DataFrame with specific columns.\n",
    "\n",
    "        :param parsed_json: Parsed JSON data.\n",
    "        :return: Pandas DataFrame sorted by the 'createdAt' column.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df_json = pd.DataFrame(parsed_json)\n",
    "            required_columns = ['name', 'createdAt', 'conclusion', 'status', 'databaseId', 'workflowDatabaseId']\n",
    "            \n",
    "            if not all(col in df_json.columns for col in required_columns):\n",
    "                raise KeyError(f\"Missing required columns in JSON data: {set(required_columns) - set(df_json.columns)}\")\n",
    "\n",
    "            df_json['createdAt'] = pd.to_datetime(df_json['createdAt'])\n",
    "            return df_json[required_columns].sort_values(by=\"createdAt\")\n",
    "        except KeyError as e:\n",
    "            raise ValueError(f\"Error processing JSON to DataFrame: {e}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Unexpected error in json_to_df: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionsWorkflow:\n",
    "    \"\"\"\n",
    "    A class to extract GitHub Actions workflows using the GitHub CLI, generating a dataframe with returned data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, repository, query_size):\n",
    "        \"\"\"\n",
    "        Initializes the ActionsWorkflow class.\n",
    "\n",
    "        :param repository: GitHub repository in the format \"owner/repo\".\n",
    "        :param query_size: Number of workflows to retrieve.\n",
    "        \"\"\"\n",
    "        self.repository = repository\n",
    "        self.json_attributes = '--json name,status,conclusion,createdAt,databaseId,workflowDatabaseId'\n",
    "        self.query_size = query_size\n",
    "        self.df = self.__gh_list_query__()\n",
    "\n",
    "    def __gh_list_query__(self):\n",
    "        \"\"\"\n",
    "        Calls the GitHub API via the GitHub CLI (`gh run list`) and retrieves\n",
    "        a specified number of workflows.\n",
    "\n",
    "        :return: A DataFrame containing the parsed workflow data.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            list_command = f'gh run --repo {self.repository} list {self.json_attributes} -L {self.query_size}'\n",
    "            \n",
    "            output_json = subprocess.run(\n",
    "                list_command, shell=True, text=True, check=True, capture_output=True\n",
    "            ).stdout\n",
    "\n",
    "            parsed_json = ArqManipulation.parse_stdout_json(output_json)\n",
    "            df = ArqManipulation.json_to_df(parsed_json)\n",
    "\n",
    "            ArqManipulation.save_df_to_parquet(df = df, parquet_file_name=\"./bin/actionsWorflow.parquet\")\n",
    "\n",
    "            return df.set_index('name')\n",
    "\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error executing GitHub CLI command: {e}\")\n",
    "            return pd.DataFrame()  # Return an empty DataFrame on error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionsJobs:\n",
    "    \"\"\"\n",
    "    A class to interact with GitHub Actions jobs using the GitHub CLI.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, repository, workflow):\n",
    "        \"\"\"\n",
    "        Initializes the ActionsJobs class.\n",
    "\n",
    "        :param repository: GitHub repository in the format \"owner/repo\".\n",
    "        :param workflow: Workflow associated with the jobs.\n",
    "        \"\"\"\n",
    "        self.repository = repository\n",
    "        self.workflow = workflow  \n",
    "\n",
    "    def get_jobs(self, database_id):\n",
    "            \"\"\"\n",
    "            Retrieves job data from the GitHub CLI and processes it.\n",
    "\n",
    "            :param database_id: The ID of the workflow run.\n",
    "            :return: A Pandas DataFrame containing job details.\n",
    "            \"\"\"\n",
    "            try:\n",
    "                jobs_df = ArqManipulation.read_parquet_file(parquet_file_name=\"./bin/actionsJobs.parquet\")\n",
    "                if jobs_df.empty or not database_id in jobs_df['Database_Id'].values:\n",
    "                    command = f'gh run --repo {self.repository} view {database_id}'\n",
    "                    display(jobs_df)\n",
    "                    jobs_data = subprocess.run(command, shell=True, text=True, check=True, capture_output=True).stdout\n",
    "                    jobs_data = self.__clean_job_text__(jobs_data)\n",
    "\n",
    "                    jobs_df = pd.concat([jobs_df, jobs_data], ignore_index=True)\n",
    "\n",
    "                    if not jobs_df.empty:\n",
    "                        jobs_df[\"Database_Id\"] = int(database_id)\n",
    "                    ArqManipulation.save_df_to_parquet(jobs_df, parquet_file_name=\"./bin/actionsJobs.parquet\")\n",
    "\n",
    "                return jobs_df\n",
    "\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"Error executing GitHub CLI command: {e}\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error: {e}\")\n",
    "                return pd.DataFrame()\n",
    "        \n",
    "    def __split_string__(self, job_list):\n",
    "        \"\"\"\n",
    "        Splits a job string into structured components.\n",
    "\n",
    "        :param job: The job string to split.\n",
    "        :return: A list of cleaned job attributes.\n",
    "        \"\"\"\n",
    "        jobs = []\n",
    "\n",
    "        for job in job_list:\n",
    "            delimiters = r\" \\| | / build in | \\(ID |\\| in| / cleanup in | /| in \" \n",
    "            splitted_job = re.split(delimiters, job)\n",
    "            splitted_job = [s.strip() for s in splitted_job if s.strip()]\n",
    "            jobs.append(splitted_job)\n",
    "        \n",
    "        jobs.pop(0)\n",
    "\n",
    "        return jobs\n",
    "\n",
    "    def __build_cleaned_df__(self, data):\n",
    "        # Define columns\n",
    "        columns = [\"conclusion\", \"test\", \"buildTime (sec)\", \"jobId\"]\n",
    "        jobs_df = pd.DataFrame(columns=columns)\n",
    "        jobs_df[\"failedAt\"] = None\n",
    "\n",
    "        for job in data:\n",
    "            if any(\"ID\" in item and (\"PASSED\" in item or \"FAILED\" in item) for item in job):\n",
    "                temp_df = pd.DataFrame(self.__split_string__(job), columns=columns)\n",
    "\n",
    "                temp_df['buildTime (sec)'] = temp_df['buildTime (sec)'].apply(str_time_to_int)\n",
    "                jobs_df = pd.concat([jobs_df, temp_df], ignore_index=True)\n",
    "            \n",
    "            elif any(\"FAILED\" in item for item in job):\n",
    "                failed = next(item for item in job if \"FAILED\" in item).split(\"FAILED | \")\n",
    "                if not jobs_df.empty:\n",
    "                    jobs_df.at[jobs_df.index[-1], \"failedAt\"] = failed[1]  \n",
    "\n",
    "        jobs_df[\"jobId\"] = jobs_df[\"jobId\"].str.rstrip(\")\").astype('int')\n",
    "        return jobs_df\n",
    "\n",
    "\n",
    "    def __find_jobs__(self, base_str: str) -> list[str]:\n",
    "        lines = base_str.splitlines()\n",
    "        arr = []  # Stores grouped sections\n",
    "        current_group = []  # Temporary storage for the current section\n",
    "\n",
    "        for line in lines:\n",
    "            if line.isupper() or not line.strip():  # New section (uppercase or empty line)\n",
    "                if current_group:  # Avoid adding empty groups\n",
    "                    arr.append(current_group)\n",
    "                current_group = [line]  # Start a new group\n",
    "            else:\n",
    "                current_group.append(line)\n",
    "\n",
    "        if current_group:  # Append the last group\n",
    "            arr.append(current_group)\n",
    "\n",
    "        # Filter out groups that do not start with an uppercase title\n",
    "        filtered_arr = [group for group in arr if group and group[0].isupper()]\n",
    "        return filtered_arr\n",
    "\n",
    "    def __clean_job_text__(self, base_str: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Cleans and structures GitHub job data from CLI output.\n",
    "\n",
    "        :param base_str: Raw job text output from the GitHub CLI.\n",
    "        :return: A Pandas DataFrame with structured job data.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Remove ANSI escape sequences and unwanted characters\n",
    "            ansi_cleaned = ArqManipulation.clean_ansi_escape(base_str)\n",
    "            cleaned = ansi_cleaned.replace(\"✓\", \"PASSED |\").replace(\"X\", \"FAILED |\")\n",
    "\n",
    "            \n",
    "            stripped_list = self.__find_jobs__(cleaned)\n",
    "\n",
    "            if any('JOBS' not in x[0] and 'ANNOTATIONS' not in x[0] for x in stripped_list):\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            jobs_df = self.__build_cleaned_df__(stripped_list)\n",
    "\n",
    "            return jobs_df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing job text: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    \n",
    "\n",
    "def str_time_to_int(time_str):\n",
    "    names = ['d', 'h', 'm', 's']\n",
    "    seconds = [86400, 3600, 60, 1]\n",
    "\n",
    "    total_time = 0\n",
    "\n",
    "    for m, t in zip(names,seconds):\n",
    "        if m in time_str:\n",
    "            time_list = time_str.split(m)\n",
    "            total_time +=  int(time_list[0]) * t\n",
    "            time_str = time_list[1]\n",
    "\n",
    "    return total_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionsArtifacts:\n",
    "    \"\"\"\n",
    "    A class to handle downloading, retrieving, and deleting GitHub Actions artifacts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, repository: str):\n",
    "        \"\"\"\n",
    "        Initializes the ActionsArtifacts object.\n",
    "\n",
    "        :param repository: The GitHub repository in the format \"owner/repo\".\n",
    "        \"\"\"\n",
    "        self.repository = repository\n",
    "        self.folder = './artifacts/'  # Default storage dir\n",
    "        self.paths = self.retrieve_downloaded_artifacts() \n",
    "\n",
    "    def download_artifact(self, database_id: str):\n",
    "        \"\"\"\n",
    "        Downloads an artifact from GitHub Actions using the GitHub CLI.\n",
    "\n",
    "        :param database_id: The database ID of the artifact to download.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Ensure the folder exists before downloading\n",
    "            os.makedirs(self.folder, exist_ok=True)\n",
    "\n",
    "            # Construct the command to download the artifact\n",
    "            command = f'gh run --repo {self.repository} download {database_id} --dir {os.path.join(self.folder, str(database_id))}'\n",
    "\n",
    "            # Execute the command\n",
    "            subprocess.run(command, shell=True, text=True, check=True)\n",
    "            print(\"Download Successful\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error during artifact download: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "\n",
    "    def retrieve_downloaded_artifacts(self) -> list[str]:\n",
    "        \"\"\"\n",
    "        Retrieves all downloaded artifacts file paths.\n",
    "\n",
    "        :return: returns Paths of the downloaded artifacts\n",
    "        \"\"\"\n",
    "        paths = []\n",
    "\n",
    "        # Walk through the artifacts folder and collect all file paths\n",
    "        for path, _, files in os.walk(self.folder):\n",
    "            for file in files:\n",
    "                paths.append(os.path.join(path, file))\n",
    "\n",
    "        return paths\n",
    "\n",
    "    def delete_downloaded_artifacts(self):\n",
    "        \"\"\"\n",
    "        Deletes all downloaded artifacts recursively\n",
    "        \"\"\"\n",
    "        try:\n",
    "            shutil.rmtree(self.folder)\n",
    "            if os.path.exists(self.folder):\n",
    "                print(\"Error: Failed to delete artifacts directory.\")\n",
    "            else:\n",
    "                print(\"Artifacts directory deleted successfully.\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"Artifacts directory not found, nothing to delete.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error while deleting artifacts: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PytestArtifactLogExtractor:\n",
    "    \"\"\"\n",
    "    A class to extract and process test status and timing information from a pytest artifact log.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path: str):\n",
    "        \"\"\"\n",
    "        Initializes the PytestArtifactLogExtractor object.\n",
    "\n",
    "        :param path: Path to the pytest artifact log file.\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        self.data = self.__read_file__()\n",
    "\n",
    "    def __read_file__(self):\n",
    "        \"\"\"\n",
    "        Reads the contents of the log file and returns it as a string.\n",
    "\n",
    "        :return: String containing the file content.\n",
    "        \"\"\"\n",
    "        with open(self.path, \"r\") as file: \n",
    "            data = file.read()\n",
    "\n",
    "        return data\n",
    "\n",
    "    def log_to_df(self):\n",
    "        \"\"\"\n",
    "        Parses the log file to extract test results and performance metrics.\n",
    "\n",
    "        :return: A DataFrame combining test statuses with time metrics.\n",
    "        \"\"\"\n",
    "        # Splitting log data based on \"=\" or new lines, while stripping whitespace\n",
    "        splitted_data = [s.strip() for s in re.split(r\"=|\\n\", self.data) if s.strip()]\n",
    "        tests = []\n",
    "\n",
    "        # Extracting test names and statuses (PASSED, ERROR, FAILED)\n",
    "        for line in splitted_data:\n",
    "            if 'PASSED' in line or 'ERROR' in line or 'FAILED' in line:\n",
    "                splitting = re.split(r\"%]| |::\", line)[-3:]  # Extracting relevant parts\n",
    "                tests.append(splitting)\n",
    "\n",
    "        # Creating a DataFrame for test statuses\n",
    "        status_df = pd.DataFrame(tests, columns=[\"status\", \"category\", \"name\"])\n",
    "        status_df = status_df.set_index('name')\n",
    "\n",
    "        # Extracting time-related metrics from the log\n",
    "        times_df = self.__extract_time_metrics__()\n",
    "\n",
    "        # Merging the test status DataFrame with the time metrics DataFrame\n",
    "        return self.__merge_artifact_dfs__(times_df=times_df, status_df=status_df)\n",
    "                \n",
    "    def __extract_time_metrics__(self):\n",
    "        \"\"\"\n",
    "        Extracts test execution time metrics from the log file.\n",
    "\n",
    "        :return: A list of DataFrames containing various test timing metrics.\n",
    "        \"\"\"\n",
    "        values = self.data.splitlines()\n",
    "        header = []\n",
    "            \n",
    "        for value in values:\n",
    "            if 'grand total' in value or 'passed in' in value:\n",
    "                continue    \n",
    "            elif '=' in value: # Divide by headers demarked by '='\n",
    "                value = value.replace(\"=\", \"\")  \n",
    "                header.append([value]) \n",
    "            else:\n",
    "                value_split = re.split(r\"\\s+\", value) \n",
    "                if header:  \n",
    "                    header[-1].append(value_split)  # Append data to the last added section\n",
    "\n",
    "\n",
    "        header.pop(0)  # Removing the first section, which contains test statuses\n",
    "\n",
    "        # Convert extracted values into DataFrames\n",
    "        time_dfs = self.__create_df__(header)\n",
    "\n",
    "        return time_dfs\n",
    "\n",
    "    def __create_df__(self, values):\n",
    "        \"\"\"\n",
    "        Converts extracted timing information into DataFrames.\n",
    "\n",
    "        :param values: A list of lists containing extracted time metrics.\n",
    "        :return: A list of DataFrames with execution time statistics.\n",
    "        \"\"\"\n",
    "        dfs = []\n",
    "        \n",
    "        for h in values:\n",
    "            headers = list(filter(None, h[1]))  # Removing empty strings from headers\n",
    "            time_df = pd.DataFrame(h[2:], columns=headers)\n",
    "\n",
    "            if 'name' in time_df.columns:\n",
    "                time_df = time_df.set_index('name') \n",
    "            \n",
    "            # Converting time-related columns to datetime.time format\n",
    "            time_columns = ['avg', 'min', 'total']\n",
    "            for col in time_columns:\n",
    "                if col in time_df.columns:\n",
    "                    time_df[col] = pd.to_datetime(time_df[col], format=\"%H:%M:%S.%f\", errors='coerce').dt.time  \n",
    "\n",
    "            # Assigning a 'durationType' column for metric categorization\n",
    "            time_df['durationType'] = h[0].replace('top', '').replace('test', '')\n",
    "\n",
    "            dfs.append(time_df.copy()) \n",
    "\n",
    "        return dfs\n",
    "\n",
    "    def __extract_self_path_info__(self):\n",
    "        \"\"\"\n",
    "        Extracts test and database ID information from the log file path.\n",
    "\n",
    "        :return: A DataFrame containing 'test' and 'databaseId' information.\n",
    "        \"\"\"\n",
    "        stripped = self.path.split('/')[-1].split('.') \n",
    "        stripped.pop(len(stripped)-1)  # Removing the file extension\n",
    "\n",
    "        return pd.DataFrame(stripped, index=['test', 'databaseId']).T\n",
    "\n",
    "    def __merge_artifact_dfs__(self, times_df, status_df):\n",
    "        \"\"\"\n",
    "        Merges test execution time data with test status information.\n",
    "\n",
    "        :param times_df: A list of DataFrames containing time-related data.\n",
    "        :param status_df: A DataFrame containing test statuses.\n",
    "        :return: A combined DataFrame containing execution metrics and test results.\n",
    "        \"\"\"\n",
    "        databaseId_df = self.__extract_self_path_info__()  \n",
    "        order = ['category', 'durationType', 'databaseId', 'status', 'num', 'avg', 'min', 'total']\n",
    "        dfs = []\n",
    "\n",
    "        for h in times_df:\n",
    "            joined_df = h.join(status_df)  # Merging time metrics with test statuses\n",
    "\n",
    "            # Adding database ID to each row\n",
    "            for col in databaseId_df.columns.values:\n",
    "                joined_df[col] = databaseId_df[col].values[0]  \n",
    "\n",
    "            # Reordering columns\n",
    "            joined_df = joined_df[order]  \n",
    "            dfs.append(joined_df)\n",
    "\n",
    "        return pd.concat(dfs)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Main\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame successfully saved to ./bin/actionsWorflow.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>createdAt</th>\n",
       "      <th>conclusion</th>\n",
       "      <th>status</th>\n",
       "      <th>databaseId</th>\n",
       "      <th>workflowDatabaseId</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pull Request Extra Tests</th>\n",
       "      <td>2025-02-06 20:58:54+00:00</td>\n",
       "      <td>failure</td>\n",
       "      <td>completed</td>\n",
       "      <td>13187766356</td>\n",
       "      <td>142271933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pull Request Essential Tests</th>\n",
       "      <td>2025-02-06 20:58:54+00:00</td>\n",
       "      <td>success</td>\n",
       "      <td>completed</td>\n",
       "      <td>13187766354</td>\n",
       "      <td>132962917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pull Request Extra Tests</th>\n",
       "      <td>2025-02-06 21:15:14+00:00</td>\n",
       "      <td>success</td>\n",
       "      <td>completed</td>\n",
       "      <td>13188025727</td>\n",
       "      <td>142271933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pull Request Essential Tests</th>\n",
       "      <td>2025-02-06 21:15:14+00:00</td>\n",
       "      <td>success</td>\n",
       "      <td>completed</td>\n",
       "      <td>13188025723</td>\n",
       "      <td>132962917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pull Request Essential Tests</th>\n",
       "      <td>2025-02-06 21:17:53+00:00</td>\n",
       "      <td>success</td>\n",
       "      <td>completed</td>\n",
       "      <td>13188066466</td>\n",
       "      <td>132962917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pull Request Extra Tests</th>\n",
       "      <td>2025-02-06 21:17:53+00:00</td>\n",
       "      <td>success</td>\n",
       "      <td>completed</td>\n",
       "      <td>13188066458</td>\n",
       "      <td>142271933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pull Request Extra Tests</th>\n",
       "      <td>2025-02-07 13:53:28+00:00</td>\n",
       "      <td>success</td>\n",
       "      <td>completed</td>\n",
       "      <td>13201233410</td>\n",
       "      <td>142271933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pull Request Essential Tests</th>\n",
       "      <td>2025-02-07 13:53:28+00:00</td>\n",
       "      <td>success</td>\n",
       "      <td>completed</td>\n",
       "      <td>13201233407</td>\n",
       "      <td>132962917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Manual Test</th>\n",
       "      <td>2025-02-07 14:39:33+00:00</td>\n",
       "      <td>failure</td>\n",
       "      <td>completed</td>\n",
       "      <td>13202033806</td>\n",
       "      <td>133121754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pages build and deployment</th>\n",
       "      <td>2025-02-07 17:03:04+00:00</td>\n",
       "      <td>success</td>\n",
       "      <td>completed</td>\n",
       "      <td>13204479513</td>\n",
       "      <td>124398581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             createdAt conclusion     status  \\\n",
       "name                                                                           \n",
       "Pull Request Extra Tests     2025-02-06 20:58:54+00:00    failure  completed   \n",
       "Pull Request Essential Tests 2025-02-06 20:58:54+00:00    success  completed   \n",
       "Pull Request Extra Tests     2025-02-06 21:15:14+00:00    success  completed   \n",
       "Pull Request Essential Tests 2025-02-06 21:15:14+00:00    success  completed   \n",
       "Pull Request Essential Tests 2025-02-06 21:17:53+00:00    success  completed   \n",
       "Pull Request Extra Tests     2025-02-06 21:17:53+00:00    success  completed   \n",
       "Pull Request Extra Tests     2025-02-07 13:53:28+00:00    success  completed   \n",
       "Pull Request Essential Tests 2025-02-07 13:53:28+00:00    success  completed   \n",
       "Manual Test                  2025-02-07 14:39:33+00:00    failure  completed   \n",
       "pages build and deployment   2025-02-07 17:03:04+00:00    success  completed   \n",
       "\n",
       "                               databaseId  workflowDatabaseId  \n",
       "name                                                           \n",
       "Pull Request Extra Tests      13187766356           142271933  \n",
       "Pull Request Essential Tests  13187766354           132962917  \n",
       "Pull Request Extra Tests      13188025727           142271933  \n",
       "Pull Request Essential Tests  13188025723           132962917  \n",
       "Pull Request Essential Tests  13188066466           132962917  \n",
       "Pull Request Extra Tests      13188066458           142271933  \n",
       "Pull Request Extra Tests      13201233410           142271933  \n",
       "Pull Request Essential Tests  13201233407           132962917  \n",
       "Manual Test                   13202033806           133121754  \n",
       "pages build and deployment    13204479513           124398581  "
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_path = 'MagaluCloud/s3-specs'\n",
    "query_size = 10\n",
    "\n",
    "workflow = ActionsWorkflow(repository=repo_path, query_size=query_size)\n",
    "workflow.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conclusion</th>\n",
       "      <th>test</th>\n",
       "      <th>buildTime (sec)</th>\n",
       "      <th>jobId</th>\n",
       "      <th>failedAt</th>\n",
       "      <th>Database_Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>run_tests (cold_storage, ../params.example.yaml)</td>\n",
       "      <td>80</td>\n",
       "      <td>36814105275</td>\n",
       "      <td>None</td>\n",
       "      <td>13187766354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>run_tests (presign, ../params.example.yaml)</td>\n",
       "      <td>71</td>\n",
       "      <td>36814105702</td>\n",
       "      <td>None</td>\n",
       "      <td>13187766354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>run_tests (basic, ../params.example.yaml)</td>\n",
       "      <td>93</td>\n",
       "      <td>36814106142</td>\n",
       "      <td>None</td>\n",
       "      <td>13187766354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>tests-success</td>\n",
       "      <td>0</td>\n",
       "      <td>36814197287</td>\n",
       "      <td>None</td>\n",
       "      <td>13187766354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>cleanup_tests</td>\n",
       "      <td>157</td>\n",
       "      <td>36814197663</td>\n",
       "      <td>None</td>\n",
       "      <td>13187766354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  conclusion                                              test  \\\n",
       "0     PASSED  run_tests (cold_storage, ../params.example.yaml)   \n",
       "1     PASSED       run_tests (presign, ../params.example.yaml)   \n",
       "2     PASSED         run_tests (basic, ../params.example.yaml)   \n",
       "3     PASSED                                     tests-success   \n",
       "4     PASSED                                     cleanup_tests   \n",
       "\n",
       "   buildTime (sec)        jobId failedAt  Database_Id  \n",
       "0               80  36814105275     None  13187766354  \n",
       "1               71  36814105702     None  13187766354  \n",
       "2               93  36814106142     None  13187766354  \n",
       "3                0  36814197287     None  13187766354  \n",
       "4              157  36814197663     None  13187766354  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing job text: argument of type 'NoneType' is not iterable\n",
      "DataFrame successfully saved to ./bin/actionsJobs.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conclusion</th>\n",
       "      <th>test</th>\n",
       "      <th>buildTime (sec)</th>\n",
       "      <th>jobId</th>\n",
       "      <th>failedAt</th>\n",
       "      <th>Database_Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>run_tests (cold_storage, ../params.example.yaml)</td>\n",
       "      <td>80</td>\n",
       "      <td>36814105275</td>\n",
       "      <td>None</td>\n",
       "      <td>13202033806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>run_tests (presign, ../params.example.yaml)</td>\n",
       "      <td>71</td>\n",
       "      <td>36814105702</td>\n",
       "      <td>None</td>\n",
       "      <td>13202033806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>run_tests (basic, ../params.example.yaml)</td>\n",
       "      <td>93</td>\n",
       "      <td>36814106142</td>\n",
       "      <td>None</td>\n",
       "      <td>13202033806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>tests-success</td>\n",
       "      <td>0</td>\n",
       "      <td>36814197287</td>\n",
       "      <td>None</td>\n",
       "      <td>13202033806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>cleanup_tests</td>\n",
       "      <td>157</td>\n",
       "      <td>36814197663</td>\n",
       "      <td>None</td>\n",
       "      <td>13202033806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  conclusion                                              test  \\\n",
       "0     PASSED  run_tests (cold_storage, ../params.example.yaml)   \n",
       "1     PASSED       run_tests (presign, ../params.example.yaml)   \n",
       "2     PASSED         run_tests (basic, ../params.example.yaml)   \n",
       "3     PASSED                                     tests-success   \n",
       "4     PASSED                                     cleanup_tests   \n",
       "\n",
       "   buildTime (sec)        jobId failedAt  Database_Id  \n",
       "0               80  36814105275     None  13202033806  \n",
       "1               71  36814105702     None  13202033806  \n",
       "2               93  36814106142     None  13202033806  \n",
       "3                0  36814197287     None  13202033806  \n",
       "4              157  36814197663     None  13202033806  "
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs = ActionsJobs(repo_path, workflow)\n",
    "j = jobs.get_jobs(13187766354)\n",
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./artifacts/13160019050/output_artifact_not_cli_and_locking_se1.13160019050/pytest_output_not_cli_and_locking_se1.13160019050.log',\n",
       " './artifacts/13160019050/output_artifact_not_cli_and_locking_ne1.13160019050/pytest_output_not_cli_and_locking_ne1.13160019050.log']"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artifacts = ActionsArtifacts(repository=repo_path)\n",
    "a = artifacts.retrieve_downloaded_artifacts()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>durationType</th>\n",
       "      <th>databaseId</th>\n",
       "      <th>status</th>\n",
       "      <th>num</th>\n",
       "      <th>avg</th>\n",
       "      <th>min</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>versioned_bucket_with_one_object</th>\n",
       "      <td>NaN</td>\n",
       "      <td>fixture duration</td>\n",
       "      <td>13160019050</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>00:00:06.876868</td>\n",
       "      <td>00:00:06.814098</td>\n",
       "      <td>00:00:20.570232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>versioned_bucket_with_lock_config</th>\n",
       "      <td>NaN</td>\n",
       "      <td>fixture duration</td>\n",
       "      <td>13160019050</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>00:00:04.122805</td>\n",
       "      <td>00:00:03.946036</td>\n",
       "      <td>00:00:12.286554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lockeable_bucket_name</th>\n",
       "      <td>NaN</td>\n",
       "      <td>fixture duration</td>\n",
       "      <td>13160019050</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>00:00:02.321122</td>\n",
       "      <td>00:00:02.321122</td>\n",
       "      <td>00:00:02.321122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>existing_bucket_name</th>\n",
       "      <td>NaN</td>\n",
       "      <td>fixture duration</td>\n",
       "      <td>13160019050</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>00:00:01.900853</td>\n",
       "      <td>00:00:01.900853</td>\n",
       "      <td>00:00:01.900853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s3_client</th>\n",
       "      <td>NaN</td>\n",
       "      <td>fixture duration</td>\n",
       "      <td>13160019050</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>00:00:00.079812</td>\n",
       "      <td>00:00:00.065131</td>\n",
       "      <td>00:00:00.846908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bucket_with_lock</th>\n",
       "      <td>NaN</td>\n",
       "      <td>fixture duration</td>\n",
       "      <td>13160019050</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>00:00:00.399650</td>\n",
       "      <td>00:00:00.399650</td>\n",
       "      <td>00:00:00.399650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_verify_object_retention</th>\n",
       "      <td>locking_test.py</td>\n",
       "      <td>call duration</td>\n",
       "      <td>13160019050</td>\n",
       "      <td>PASSED</td>\n",
       "      <td>1</td>\n",
       "      <td>00:00:01.592751</td>\n",
       "      <td>00:00:01.592751</td>\n",
       "      <td>00:00:01.592751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_verify_object_lock_configuration</th>\n",
       "      <td>locking_test.py</td>\n",
       "      <td>call duration</td>\n",
       "      <td>13160019050</td>\n",
       "      <td>PASSED</td>\n",
       "      <td>1</td>\n",
       "      <td>00:00:01.329674</td>\n",
       "      <td>00:00:01.329674</td>\n",
       "      <td>00:00:01.329674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_simple_delete_with_lock</th>\n",
       "      <td>locking_test.py</td>\n",
       "      <td>call duration</td>\n",
       "      <td>13160019050</td>\n",
       "      <td>PASSED</td>\n",
       "      <td>1</td>\n",
       "      <td>00:00:00.919780</td>\n",
       "      <td>00:00:00.919780</td>\n",
       "      <td>00:00:00.919780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_delete_object_after_locking</th>\n",
       "      <td>locking_test.py</td>\n",
       "      <td>call duration</td>\n",
       "      <td>13160019050</td>\n",
       "      <td>PASSED</td>\n",
       "      <td>1</td>\n",
       "      <td>00:00:00.735536</td>\n",
       "      <td>00:00:00.735536</td>\n",
       "      <td>00:00:00.735536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_configure_bucket_lock_on_regular_bucket</th>\n",
       "      <td>locking_test.py</td>\n",
       "      <td>call duration</td>\n",
       "      <td>13160019050</td>\n",
       "      <td>PASSED</td>\n",
       "      <td>1</td>\n",
       "      <td>00:00:00.316684</td>\n",
       "      <td>00:00:00.316684</td>\n",
       "      <td>00:00:00.316684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_simple_delete_with_lock</th>\n",
       "      <td>locking_test.py</td>\n",
       "      <td>setup duration</td>\n",
       "      <td>13160019050</td>\n",
       "      <td>PASSED</td>\n",
       "      <td>1</td>\n",
       "      <td>00:00:11.141857</td>\n",
       "      <td>00:00:11.141857</td>\n",
       "      <td>00:00:11.141857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_delete_object_after_locking</th>\n",
       "      <td>locking_test.py</td>\n",
       "      <td>setup duration</td>\n",
       "      <td>13160019050</td>\n",
       "      <td>PASSED</td>\n",
       "      <td>1</td>\n",
       "      <td>00:00:11.100906</td>\n",
       "      <td>00:00:11.100906</td>\n",
       "      <td>00:00:11.100906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_verify_object_retention</th>\n",
       "      <td>locking_test.py</td>\n",
       "      <td>setup duration</td>\n",
       "      <td>13160019050</td>\n",
       "      <td>PASSED</td>\n",
       "      <td>1</td>\n",
       "      <td>00:00:11.083300</td>\n",
       "      <td>00:00:11.083300</td>\n",
       "      <td>00:00:11.083300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_verify_object_lock_configuration</th>\n",
       "      <td>locking_test.py</td>\n",
       "      <td>setup duration</td>\n",
       "      <td>13160019050</td>\n",
       "      <td>PASSED</td>\n",
       "      <td>1</td>\n",
       "      <td>00:00:03.039559</td>\n",
       "      <td>00:00:03.039559</td>\n",
       "      <td>00:00:03.039559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_configure_bucket_lock_on_regular_bucket</th>\n",
       "      <td>locking_test.py</td>\n",
       "      <td>setup duration</td>\n",
       "      <td>13160019050</td>\n",
       "      <td>PASSED</td>\n",
       "      <td>1</td>\n",
       "      <td>00:00:01.967182</td>\n",
       "      <td>00:00:01.967182</td>\n",
       "      <td>00:00:01.967182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_verify_object_retention</th>\n",
       "      <td>locking_test.py</td>\n",
       "      <td>teardown duration</td>\n",
       "      <td>13160019050</td>\n",
       "      <td>PASSED</td>\n",
       "      <td>1</td>\n",
       "      <td>00:00:26.163196</td>\n",
       "      <td>00:00:26.163196</td>\n",
       "      <td>00:00:26.163196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_simple_delete_with_lock</th>\n",
       "      <td>locking_test.py</td>\n",
       "      <td>teardown duration</td>\n",
       "      <td>13160019050</td>\n",
       "      <td>PASSED</td>\n",
       "      <td>1</td>\n",
       "      <td>00:00:26.126737</td>\n",
       "      <td>00:00:26.126737</td>\n",
       "      <td>00:00:26.126737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_delete_object_after_locking</th>\n",
       "      <td>locking_test.py</td>\n",
       "      <td>teardown duration</td>\n",
       "      <td>13160019050</td>\n",
       "      <td>PASSED</td>\n",
       "      <td>1</td>\n",
       "      <td>00:00:25.723083</td>\n",
       "      <td>00:00:25.723083</td>\n",
       "      <td>00:00:25.723083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_verify_object_lock_configuration</th>\n",
       "      <td>locking_test.py</td>\n",
       "      <td>teardown duration</td>\n",
       "      <td>13160019050</td>\n",
       "      <td>PASSED</td>\n",
       "      <td>1</td>\n",
       "      <td>00:00:07.102668</td>\n",
       "      <td>00:00:07.102668</td>\n",
       "      <td>00:00:07.102668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_configure_bucket_lock_on_regular_bucket</th>\n",
       "      <td>locking_test.py</td>\n",
       "      <td>teardown duration</td>\n",
       "      <td>13160019050</td>\n",
       "      <td>PASSED</td>\n",
       "      <td>1</td>\n",
       "      <td>00:00:01.190274</td>\n",
       "      <td>00:00:01.190274</td>\n",
       "      <td>00:00:01.190274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     category  \\\n",
       "name                                                            \n",
       "versioned_bucket_with_one_object                          NaN   \n",
       "versioned_bucket_with_lock_config                         NaN   \n",
       "lockeable_bucket_name                                     NaN   \n",
       "existing_bucket_name                                      NaN   \n",
       "s3_client                                                 NaN   \n",
       "bucket_with_lock                                          NaN   \n",
       "test_verify_object_retention                  locking_test.py   \n",
       "test_verify_object_lock_configuration         locking_test.py   \n",
       "test_simple_delete_with_lock                  locking_test.py   \n",
       "test_delete_object_after_locking              locking_test.py   \n",
       "test_configure_bucket_lock_on_regular_bucket  locking_test.py   \n",
       "test_simple_delete_with_lock                  locking_test.py   \n",
       "test_delete_object_after_locking              locking_test.py   \n",
       "test_verify_object_retention                  locking_test.py   \n",
       "test_verify_object_lock_configuration         locking_test.py   \n",
       "test_configure_bucket_lock_on_regular_bucket  locking_test.py   \n",
       "test_verify_object_retention                  locking_test.py   \n",
       "test_simple_delete_with_lock                  locking_test.py   \n",
       "test_delete_object_after_locking              locking_test.py   \n",
       "test_verify_object_lock_configuration         locking_test.py   \n",
       "test_configure_bucket_lock_on_regular_bucket  locking_test.py   \n",
       "\n",
       "                                                       durationType  \\\n",
       "name                                                                  \n",
       "versioned_bucket_with_one_object                 fixture duration     \n",
       "versioned_bucket_with_lock_config                fixture duration     \n",
       "lockeable_bucket_name                            fixture duration     \n",
       "existing_bucket_name                             fixture duration     \n",
       "s3_client                                        fixture duration     \n",
       "bucket_with_lock                                 fixture duration     \n",
       "test_verify_object_retention                        call duration     \n",
       "test_verify_object_lock_configuration               call duration     \n",
       "test_simple_delete_with_lock                        call duration     \n",
       "test_delete_object_after_locking                    call duration     \n",
       "test_configure_bucket_lock_on_regular_bucket        call duration     \n",
       "test_simple_delete_with_lock                       setup duration     \n",
       "test_delete_object_after_locking                   setup duration     \n",
       "test_verify_object_retention                       setup duration     \n",
       "test_verify_object_lock_configuration              setup duration     \n",
       "test_configure_bucket_lock_on_regular_bucket       setup duration     \n",
       "test_verify_object_retention                    teardown duration     \n",
       "test_simple_delete_with_lock                    teardown duration     \n",
       "test_delete_object_after_locking                teardown duration     \n",
       "test_verify_object_lock_configuration           teardown duration     \n",
       "test_configure_bucket_lock_on_regular_bucket    teardown duration     \n",
       "\n",
       "                                               databaseId  status num  \\\n",
       "name                                                                    \n",
       "versioned_bucket_with_one_object              13160019050     NaN   3   \n",
       "versioned_bucket_with_lock_config             13160019050     NaN   3   \n",
       "lockeable_bucket_name                         13160019050     NaN   1   \n",
       "existing_bucket_name                          13160019050     NaN   1   \n",
       "s3_client                                     13160019050     NaN   5   \n",
       "bucket_with_lock                              13160019050     NaN   1   \n",
       "test_verify_object_retention                  13160019050  PASSED   1   \n",
       "test_verify_object_lock_configuration         13160019050  PASSED   1   \n",
       "test_simple_delete_with_lock                  13160019050  PASSED   1   \n",
       "test_delete_object_after_locking              13160019050  PASSED   1   \n",
       "test_configure_bucket_lock_on_regular_bucket  13160019050  PASSED   1   \n",
       "test_simple_delete_with_lock                  13160019050  PASSED   1   \n",
       "test_delete_object_after_locking              13160019050  PASSED   1   \n",
       "test_verify_object_retention                  13160019050  PASSED   1   \n",
       "test_verify_object_lock_configuration         13160019050  PASSED   1   \n",
       "test_configure_bucket_lock_on_regular_bucket  13160019050  PASSED   1   \n",
       "test_verify_object_retention                  13160019050  PASSED   1   \n",
       "test_simple_delete_with_lock                  13160019050  PASSED   1   \n",
       "test_delete_object_after_locking              13160019050  PASSED   1   \n",
       "test_verify_object_lock_configuration         13160019050  PASSED   1   \n",
       "test_configure_bucket_lock_on_regular_bucket  13160019050  PASSED   1   \n",
       "\n",
       "                                                          avg  \\\n",
       "name                                                            \n",
       "versioned_bucket_with_one_object              00:00:06.876868   \n",
       "versioned_bucket_with_lock_config             00:00:04.122805   \n",
       "lockeable_bucket_name                         00:00:02.321122   \n",
       "existing_bucket_name                          00:00:01.900853   \n",
       "s3_client                                     00:00:00.079812   \n",
       "bucket_with_lock                              00:00:00.399650   \n",
       "test_verify_object_retention                  00:00:01.592751   \n",
       "test_verify_object_lock_configuration         00:00:01.329674   \n",
       "test_simple_delete_with_lock                  00:00:00.919780   \n",
       "test_delete_object_after_locking              00:00:00.735536   \n",
       "test_configure_bucket_lock_on_regular_bucket  00:00:00.316684   \n",
       "test_simple_delete_with_lock                  00:00:11.141857   \n",
       "test_delete_object_after_locking              00:00:11.100906   \n",
       "test_verify_object_retention                  00:00:11.083300   \n",
       "test_verify_object_lock_configuration         00:00:03.039559   \n",
       "test_configure_bucket_lock_on_regular_bucket  00:00:01.967182   \n",
       "test_verify_object_retention                  00:00:26.163196   \n",
       "test_simple_delete_with_lock                  00:00:26.126737   \n",
       "test_delete_object_after_locking              00:00:25.723083   \n",
       "test_verify_object_lock_configuration         00:00:07.102668   \n",
       "test_configure_bucket_lock_on_regular_bucket  00:00:01.190274   \n",
       "\n",
       "                                                          min            total  \n",
       "name                                                                            \n",
       "versioned_bucket_with_one_object              00:00:06.814098  00:00:20.570232  \n",
       "versioned_bucket_with_lock_config             00:00:03.946036  00:00:12.286554  \n",
       "lockeable_bucket_name                         00:00:02.321122  00:00:02.321122  \n",
       "existing_bucket_name                          00:00:01.900853  00:00:01.900853  \n",
       "s3_client                                     00:00:00.065131  00:00:00.846908  \n",
       "bucket_with_lock                              00:00:00.399650  00:00:00.399650  \n",
       "test_verify_object_retention                  00:00:01.592751  00:00:01.592751  \n",
       "test_verify_object_lock_configuration         00:00:01.329674  00:00:01.329674  \n",
       "test_simple_delete_with_lock                  00:00:00.919780  00:00:00.919780  \n",
       "test_delete_object_after_locking              00:00:00.735536  00:00:00.735536  \n",
       "test_configure_bucket_lock_on_regular_bucket  00:00:00.316684  00:00:00.316684  \n",
       "test_simple_delete_with_lock                  00:00:11.141857  00:00:11.141857  \n",
       "test_delete_object_after_locking              00:00:11.100906  00:00:11.100906  \n",
       "test_verify_object_retention                  00:00:11.083300  00:00:11.083300  \n",
       "test_verify_object_lock_configuration         00:00:03.039559  00:00:03.039559  \n",
       "test_configure_bucket_lock_on_regular_bucket  00:00:01.967182  00:00:01.967182  \n",
       "test_verify_object_retention                  00:00:26.163196  00:00:26.163196  \n",
       "test_simple_delete_with_lock                  00:00:26.126737  00:00:26.126737  \n",
       "test_delete_object_after_locking              00:00:25.723083  00:00:25.723083  \n",
       "test_verify_object_lock_configuration         00:00:07.102668  00:00:07.102668  \n",
       "test_configure_bucket_lock_on_regular_bucket  00:00:01.190274  00:00:01.190274  "
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artifact = PytestArtifactLogExtractor(path = a[1])\n",
    "b = artifact.log_to_df()\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Faz o teste -> gera graficos com tempo e taxa de falhas por tipo de teste\n",
    "\n",
    "Workflow -> Job -> Passos -> Resultados pytest \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define color mapping\n",
    "colors = {\n",
    "    'failure': 'firebrick',\n",
    "    'cancelled': 'darkgray',\n",
    "    'startup_failure': 'darkorange',\n",
    "    'success':  'darkgreen'\n",
    "}\n",
    "\n",
    "# Filter the DataFrame\n",
    "a = workflow.df[workflow.df['status'] == 'completed']\n",
    "\n",
    "# Get value counts of the 'conclusion' columnimport matplotlib.pyplot as plt\n",
    "\n",
    "# Define color mapping\n",
    "colors = {\n",
    "    'failure': 'firebrick',\n",
    "    'cancelled': 'darkgray',\n",
    "    'startup_failure':'darkorange',\n",
    "    'success':  'darkgreen'\n",
    "\n",
    "}\n",
    "\n",
    "# Filter the DataFrame\n",
    "a = workflow.df[workflow.df['status'] == 'completed']\n",
    "\n",
    "# Get value counts of the 'conclusion' column\n",
    "value_counts = a['conclusion'].value_counts()\n",
    "\n",
    "# Map colors to the categories in value_counts\n",
    "bar_colors = [colors[cat] for cat in value_counts.index]\n",
    "\n",
    "# Create a figure with two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))  # 1 row, 2 columns\n",
    "\n",
    "# Plot the bar chart on the first subplot\n",
    "value_counts.plot.bar(color=bar_colors, ax=ax1)\n",
    "ax1.set_xlabel('Conclusion')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('Bar Chart: Conclusion Counts')\n",
    "\n",
    "# Plot the pie chart on the second subplot\n",
    "value_counts.plot.pie(colors=bar_colors, autopct='%1.1f%%', ax=ax2)\n",
    "ax2.set_ylabel('')  # Remove the y-label for the pie chart\n",
    "ax2.set_title('Pie Chart: Conclusion Distribution')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n",
    "value_counts = a['conclusion'].value_counts()\n",
    "\n",
    "# Map colors to the categories in value_counts\n",
    "bar_colors = [colors[cat] for cat in value_counts.index]\n",
    "\n",
    "# Create a figure with two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))  # 1 row, 2 columns\n",
    "\n",
    "# Plot the bar chart on the first subplot\n",
    "value_counts.plot.bar(color=bar_colors, ax=ax1)\n",
    "ax1.set_xlabel('Conclusion')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('Bar Chart: Conclusion Counts')\n",
    "\n",
    "# Plot the pie chart on the second subplot\n",
    "value_counts.plot.pie(colors=bar_colors, autopct='%1.1f%%', ax=ax2)\n",
    "ax2.set_ylabel('')  # Remove the y-label for the pie chart\n",
    "ax2.set_title('Pie Chart: Conclusion Distribution')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jobs Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = ActionsJobs(repository=repo_path, workflow=workflow)\n",
    "ids = workflow.df['databaseId'].unique()\n",
    "all_job_dfs = [jobs.get_jobs(id)for id in ids]\n",
    "jobs_df = pd.concat(all_job_dfs)\n",
    "jobs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_failed_passed_jobs_bars(df):\n",
    "    unique_names = df.groupby(['Test', 'Conclusion']).size().unstack(fill_value=0)\n",
    "    test_to_number = {test: i + 1 for i, test in enumerate(df['Test'].unique())}\n",
    "\n",
    "    # Define colors for 'FAILED' and 'PASSED'\n",
    "    colors = {\n",
    "        'FAILED': 'firebrick',\n",
    "        'PASSED': 'darkgreen'\n",
    "    }\n",
    "\n",
    "    ax = unique_names.plot.bar(color=[colors['FAILED'], colors['PASSED']], figsize=(8, 4))\n",
    "\n",
    "    # Add labels and title\n",
    "    ax.set_xlabel('Test')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('FAILED vs PASSED by Test')\n",
    "\n",
    "    # Change the x-tick labels to their respective numbers\n",
    "    ax.set_xticklabels([test_to_number[test] for test in unique_names.index], rotation=0)\n",
    "\n",
    "    # Create a legend for the test numbers and names\n",
    "    test_legend = [f\"{num}. {test}\" for test, num in test_to_number.items()]\n",
    "    plt.figtext(1.05, 0.5, \"\\n\".join(test_legend), va='center', fontsize=10, wrap=True)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "#plot_failed_passed_jobs_bars(jobs_df[jobs_df['Conclusion'] == 'FAILED'])\n",
    "plot_failed_passed_jobs_bars(jobs_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
