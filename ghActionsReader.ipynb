{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import subprocess\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArqManipulation:\n",
    "    \"\"\"\n",
    "    A utility class for file operations and data manipulation.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod \n",
    "    def read_parquet_file(parquet_file_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Reads a Parquet file and returns a DataFrame.\n",
    "\n",
    "        :param parquet_file_name: Path to the Parquet file.\n",
    "        :return: DataFrame with file contents.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(parquet_file_name):\n",
    "                print(f\"File '{parquet_file_name}' does not exist.\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            return pd.read_parquet(parquet_file_name)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error reading Parquet file '{parquet_file_name}': {e}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def save_df_to_parquet(df: pd.DataFrame, parquet_file_name: str):\n",
    "        \"\"\"\n",
    "        Saves a DataFrame to a Parquet file.\n",
    "\n",
    "        :param df: Dataframe to save.\n",
    "        :param parquet_file_name: Parqueet saving path.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(parquet_file_name), exist_ok=True)\n",
    "            df.to_parquet(parquet_file_name)\n",
    "            print(f\"DataFrame successfully saved to {parquet_file_name}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error saving DataFrame to Parquet file '{parquet_file_name}': {e}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_ansi_escape(base_str: str) -> str:\n",
    "        \"\"\"\n",
    "        Removes ANSI escape values from a string.\n",
    "\n",
    "        :param base_str: Unformmated string.\n",
    "        :return: Cleaned string.\n",
    "        \"\"\"\n",
    "        return re.sub(r'\\x1B\\[[0-9;]*[A-Za-z]', '', base_str)\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_stdout_json(base_str: str) -> dict:\n",
    "        \"\"\"\n",
    "        Parses JSON output from GitHub CLI after cleaning ANSI escape sequences.\n",
    "\n",
    "        :param base_str: The raw output string from the GitHub CLI.\n",
    "        :return: Parsed JSON dictionary.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            cleaned = ArqManipulation.clean_ansi_escape(base_str)\n",
    "            str_output = ''.join(cleaned.splitlines())\n",
    "            return json.loads(str_output)\n",
    "        except json.JSONDecodeError as e:\n",
    "            raise e\n",
    "\n",
    "    @staticmethod\n",
    "    def json_to_df(parsed_json: dict) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Converts a JSON dictionary to a sorted DataFrame with specific columns.\n",
    "\n",
    "        :param parsed_json: Parsed JSON data.\n",
    "        :return: Pandas DataFrame sorted by the 'createdAt' column.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df_json = pd.DataFrame(parsed_json)\n",
    "            required_columns = ['name', 'createdAt', 'conclusion', 'status', 'databaseId', 'workflowDatabaseId']\n",
    "            \n",
    "            if not all(col in df_json.columns for col in required_columns):\n",
    "                raise KeyError(f\"Missing required columns in JSON data: {set(required_columns) - set(df_json.columns)}\")\n",
    "\n",
    "            df_json['createdAt'] = pd.to_datetime(df_json['createdAt'])\n",
    "            return df_json[required_columns].sort_values(by=\"createdAt\")\n",
    "        except KeyError as e:\n",
    "            raise ValueError(f\"Error processing JSON to DataFrame: {e}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Unexpected error in json_to_df: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionsWorkflow:\n",
    "    \"\"\"\n",
    "    A class to extract GitHub Actions workflows using the GitHub CLI, generating a dataframe with returned data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, repository, query_size):\n",
    "        \"\"\"\n",
    "        Initializes the ActionsWorkflow class.\n",
    "\n",
    "        :param repository: GitHub repository in the format \"owner/repo\".\n",
    "        :param query_size: Number of workflows to retrieve.\n",
    "        \"\"\"\n",
    "        self.repository = repository\n",
    "        self.json_attributes = '--json name,status,conclusion,createdAt,databaseId,workflowDatabaseId'\n",
    "        self.query_size = query_size\n",
    "        self.df = self.__gh_list_query__()\n",
    "\n",
    "    def __gh_list_query__(self):\n",
    "        \"\"\"\n",
    "        Calls the GitHub API via the GitHub CLI (`gh run list`) and retrieves\n",
    "        a specified number of workflows.\n",
    "\n",
    "        :return: A DataFrame containing the parsed workflow data.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            list_command = f'gh run --repo {self.repository} list {self.json_attributes} -L {self.query_size}'\n",
    "            \n",
    "            output_json = subprocess.run(\n",
    "                list_command, shell=True, text=True, check=True, capture_output=True\n",
    "            ).stdout\n",
    "\n",
    "            parsed_json = ArqManipulation.parse_stdout_json(output_json)\n",
    "            df = ArqManipulation.json_to_df(parsed_json)\n",
    "\n",
    "            ArqManipulation.save_df_to_parquet(df = df, parquet_file_name=\"./bin/actionsWorflow.parquet\")\n",
    "\n",
    "            return df\n",
    "\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error executing GitHub CLI command: {e}\")\n",
    "            return pd.DataFrame()  # Return an empty DataFrame on error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionsJobs:\n",
    "    \"\"\"\n",
    "    A class to interact with GitHub Actions jobs using the GitHub CLI.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, repository, workflow):\n",
    "        \"\"\"\n",
    "        Initializes the ActionsJobs class.\n",
    "\n",
    "        :param repository: GitHub repository in the format \"owner/repo\".\n",
    "        :param workflow: Workflow associated with the jobs.\n",
    "        \"\"\"\n",
    "        self.repository = repository\n",
    "        self.workflow = workflow  \n",
    "\n",
    "    def __split_string__(self, job):\n",
    "        \"\"\"\n",
    "        Splits a job string into structured components.\n",
    "\n",
    "        :param job: The job string to split.\n",
    "        :return: A list of cleaned job attributes.\n",
    "        \"\"\"\n",
    "        delimiters = r\" \\| | / build in | \\(ID |\\| in| / cleanup in | /\" \n",
    "        splitted_job = re.split(delimiters, job)\n",
    "        splitted_job = [s.strip() for s in splitted_job if s.strip()]\n",
    "\n",
    "        return splitted_job\n",
    "\n",
    "\n",
    "    def __clean_job_text__(self, base_str: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Cleans and structures GitHub job data from CLI output.\n",
    "\n",
    "        :param base_str: Raw job text output from the GitHub CLI.\n",
    "        :return: A Pandas DataFrame with structured job data.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Remove ANSI escape sequences and unwanted characters\n",
    "            ansi_cleaned = ArqManipulation.clean_ansi_escape(base_str)\n",
    "            cleaned = ansi_cleaned.replace(\"âœ“\", \"PASSED |\").replace(\"X\", \"FAILED |\")\n",
    "            \n",
    "            # Extract job details section\n",
    "            start_index = cleaned.find(\"JOBS\")\n",
    "            end_index = cleaned.find(\"ANNOTATIONS\")\n",
    "            if start_index == -1 or end_index == -1:\n",
    "                print(\"Warning: JOBS or ANNOTATIONS section not found in output.\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            cleaned_list = cleaned[start_index:end_index].splitlines()\n",
    "\n",
    "            # Define columns\n",
    "            columns = [\"Conclusion\", \"Test\", \"Build_Time\", \"Job_Id\"]\n",
    "            jobs_df = pd.DataFrame(columns=columns)\n",
    "            jobs_df[\"Failed_At\"] = None\n",
    "\n",
    "            for job in cleaned_list:\n",
    "                if \"ID\" in job and (\"PASSED\" in job or \"FAILED\" in job):\n",
    "                    temp_df = pd.DataFrame([self.__split_string__(job)], columns=columns)\n",
    "                    jobs_df = pd.concat([jobs_df, temp_df], ignore_index=True)\n",
    "                \n",
    "                elif \"FAILED\" in job:\n",
    "                    failed = job.split(\"FAILED | \")\n",
    "                    if not jobs_df.empty:\n",
    "                        jobs_df.at[jobs_df.index[-1], \"Failed_At\"] = failed[1]  \n",
    "\n",
    "            jobs_df[\"Job_Id\"] = jobs_df[\"Job_Id\"].str.replace(\")\", \"\", regex=False).astype('int')\n",
    "\n",
    "            return jobs_df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing job text: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def get_jobs(self, database_id):\n",
    "        \"\"\"\n",
    "        Retrieves job data from the GitHub CLI and processes it.\n",
    "\n",
    "        :param database_id: The ID of the workflow run.\n",
    "        :return: A Pandas DataFrame containing job details.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            jobs_df = ArqManipulation.read_parquet_file(parquet_file_name=\"./bin/actionsJobs.parquet\")\n",
    "            if not jobs_df.empty and database_id in jobs_df['Database_Id'].values:\n",
    "                print(\"WorkflowId already exists on parquet file\")\n",
    "                return jobs_df\n",
    "\n",
    "            command = f'gh run --repo {self.repository} view {database_id}'\n",
    "            jobs_data = subprocess.run(command, shell=True, text=True, check=True, capture_output=True).stdout\n",
    "\n",
    "            jobs_df = self.__clean_job_text__(jobs_data)\n",
    "\n",
    "            if not jobs_df.empty:\n",
    "                jobs_df[\"Database_Id\"] = int(database_id)\n",
    "\n",
    "            ArqManipulation.save_df_to_parquet(jobs_df, parquet_file_name=\"./bin/actionsJobs.parquet\")\n",
    "\n",
    "            return jobs_df\n",
    "\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error executing GitHub CLI command: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "            return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Main\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame successfully saved to ./bin/actionsWorflow.parquet\n"
     ]
    }
   ],
   "source": [
    "repo_path = 'MagaluCloud/s3-specs'\n",
    "query_size = 10\n",
    "\n",
    "workflow = ActionsWorkflow(repository=repo_path, query_size=query_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WorkflowId already exists on parquet file\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Conclusion</th>\n",
       "      <th>Test</th>\n",
       "      <th>Build_Time</th>\n",
       "      <th>Job_Id</th>\n",
       "      <th>Failed_At</th>\n",
       "      <th>Database_Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>run_extra_tests (not cli and locking, ../param...</td>\n",
       "      <td>3m21s</td>\n",
       "      <td>36500792464</td>\n",
       "      <td>None</td>\n",
       "      <td>13079876247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FAILED</td>\n",
       "      <td>run_extra_tests (not cli and locking, ../param...</td>\n",
       "      <td>13m35s</td>\n",
       "      <td>36500792820</td>\n",
       "      <td>Run tests *_test.py</td>\n",
       "      <td>13079876247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>run_extra_tests (acl, ../params/br-ne1.yaml)</td>\n",
       "      <td>16m54s</td>\n",
       "      <td>36500793109</td>\n",
       "      <td>None</td>\n",
       "      <td>13079876247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>run_extra_tests (acl, ../params/br-se1.yaml)</td>\n",
       "      <td>1h12m28s</td>\n",
       "      <td>36500793375</td>\n",
       "      <td>None</td>\n",
       "      <td>13079876247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FAILED</td>\n",
       "      <td>run_extra_tests (policy, ../params/br-ne1.yaml)</td>\n",
       "      <td>4m47s</td>\n",
       "      <td>36500793640</td>\n",
       "      <td>Run tests *_test.py</td>\n",
       "      <td>13079876247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FAILED</td>\n",
       "      <td>run_extra_tests (policy, ../params/br-se1.yaml)</td>\n",
       "      <td>57m55s</td>\n",
       "      <td>36500793871</td>\n",
       "      <td>Run tests *_test.py</td>\n",
       "      <td>13079876247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>run_tests (basic)</td>\n",
       "      <td>1m43s</td>\n",
       "      <td>36500794184</td>\n",
       "      <td>None</td>\n",
       "      <td>13079876247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FAILED</td>\n",
       "      <td>run_tests (bucket_versioning)</td>\n",
       "      <td>5m1s</td>\n",
       "      <td>36500794395</td>\n",
       "      <td>Run tests *_test.py</td>\n",
       "      <td>13079876247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>run_tests (cold_storage)</td>\n",
       "      <td>1m27s</td>\n",
       "      <td>36500794628</td>\n",
       "      <td>None</td>\n",
       "      <td>13079876247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>run_tests (presign)</td>\n",
       "      <td>1m7s</td>\n",
       "      <td>36500794875</td>\n",
       "      <td>None</td>\n",
       "      <td>13079876247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>cleanup_tests</td>\n",
       "      <td>1m38s</td>\n",
       "      <td>36503918882</td>\n",
       "      <td>None</td>\n",
       "      <td>13079876247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Conclusion                                               Test Build_Time  \\\n",
       "0      PASSED  run_extra_tests (not cli and locking, ../param...      3m21s   \n",
       "1      FAILED  run_extra_tests (not cli and locking, ../param...     13m35s   \n",
       "2      PASSED       run_extra_tests (acl, ../params/br-ne1.yaml)     16m54s   \n",
       "3      PASSED       run_extra_tests (acl, ../params/br-se1.yaml)   1h12m28s   \n",
       "4      FAILED    run_extra_tests (policy, ../params/br-ne1.yaml)      4m47s   \n",
       "5      FAILED    run_extra_tests (policy, ../params/br-se1.yaml)     57m55s   \n",
       "6      PASSED                                  run_tests (basic)      1m43s   \n",
       "7      FAILED                      run_tests (bucket_versioning)       5m1s   \n",
       "8      PASSED                           run_tests (cold_storage)      1m27s   \n",
       "9      PASSED                                run_tests (presign)       1m7s   \n",
       "10     PASSED                                      cleanup_tests      1m38s   \n",
       "\n",
       "         Job_Id            Failed_At  Database_Id  \n",
       "0   36500792464                 None  13079876247  \n",
       "1   36500792820  Run tests *_test.py  13079876247  \n",
       "2   36500793109                 None  13079876247  \n",
       "3   36500793375                 None  13079876247  \n",
       "4   36500793640  Run tests *_test.py  13079876247  \n",
       "5   36500793871  Run tests *_test.py  13079876247  \n",
       "6   36500794184                 None  13079876247  \n",
       "7   36500794395  Run tests *_test.py  13079876247  \n",
       "8   36500794628                 None  13079876247  \n",
       "9   36500794875                 None  13079876247  \n",
       "10  36503918882                 None  13079876247  "
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs = ActionsJobs(repository=repo_path, workflow=workflow)\n",
    "job_id = workflow.df['databaseId'].get(1)\n",
    "c = jobs.get_jobs(job_id)\n",
    "c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
