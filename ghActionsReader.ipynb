{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import subprocess\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "from itertools import chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArqManipulation:\n",
    "    \"\"\"\n",
    "    A utility class for file operations and data manipulation.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod \n",
    "    def read_parquet_file(parquet_file_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Reads a Parquet file and returns a DataFrame.\n",
    "\n",
    "        :param parquet_file_name: Path to the Parquet file.\n",
    "        :return: DataFrame with file contents.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(parquet_file_name):\n",
    "                print(f\"File '{parquet_file_name}' does not exist.\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            return pd.read_parquet(parquet_file_name)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error reading Parquet file '{parquet_file_name}': {e}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def save_df_to_parquet(df: pd.DataFrame, parquet_file_name: str):\n",
    "        \"\"\"\n",
    "        Saves a DataFrame to a Parquet file.\n",
    "\n",
    "        :param df: Dataframe to save.\n",
    "        :param parquet_file_name: Parqueet saving path.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(parquet_file_name), exist_ok=True)\n",
    "            df.to_parquet(parquet_file_name)\n",
    "            print(f\"DataFrame successfully saved to {parquet_file_name}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error saving DataFrame to Parquet file '{parquet_file_name}': {e}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_ansi_escape(base_str: str) -> str:\n",
    "        \"\"\"\n",
    "        Removes ANSI escape values from a string.\n",
    "\n",
    "        :param base_str: Unformmated string.\n",
    "        :return: Cleaned string.\n",
    "        \"\"\"\n",
    "        return re.sub(r'\\x1B\\[[0-9;]*[A-Za-z]', '', base_str)\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_stdout_json(base_str: str) -> dict:\n",
    "        \"\"\"\n",
    "        Parses JSON output from GitHub CLI after cleaning ANSI escape sequences.\n",
    "\n",
    "        :param base_str: The raw output string from the GitHub CLI.\n",
    "        :return: Parsed JSON dictionary.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            cleaned = ArqManipulation.clean_ansi_escape(base_str)\n",
    "            str_output = ''.join(cleaned.splitlines())\n",
    "            return json.loads(str_output)\n",
    "        except json.JSONDecodeError as e:\n",
    "            raise e\n",
    "\n",
    "    @staticmethod\n",
    "    def json_to_df(parsed_json: dict) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Converts a JSON dictionary to a sorted DataFrame with specific columns.\n",
    "\n",
    "        :param parsed_json: Parsed JSON data.\n",
    "        :return: Pandas DataFrame sorted by the 'createdAt' column.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df_json = pd.DataFrame(parsed_json)\n",
    "            required_columns = ['name', 'createdAt', 'conclusion', 'status', 'databaseId', 'workflowDatabaseId']\n",
    "            \n",
    "            if not all(col in df_json.columns for col in required_columns):\n",
    "                raise KeyError(f\"Missing required columns in JSON data: {set(required_columns) - set(df_json.columns)}\")\n",
    "\n",
    "            df_json['createdAt'] = pd.to_datetime(df_json['createdAt'])\n",
    "            return df_json[required_columns].sort_values(by=\"createdAt\")\n",
    "        except KeyError as e:\n",
    "            raise ValueError(f\"Error processing JSON to DataFrame: {e}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Unexpected error in json_to_df: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionsWorkflow:\n",
    "    \"\"\"\n",
    "    A class to extract GitHub Actions workflows using the GitHub CLI, generating a dataframe with returned data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, repository, query_size):\n",
    "        \"\"\"\n",
    "        Initializes the ActionsWorkflow class.\n",
    "\n",
    "        :param repository: GitHub repository in the format \"owner/repo\".\n",
    "        :param query_size: Number of workflows to retrieve.\n",
    "        \"\"\"\n",
    "        self.repository = repository\n",
    "        self.json_attributes = '--json name,status,conclusion,createdAt,databaseId,workflowDatabaseId'\n",
    "        self.query_size = query_size\n",
    "        self.df = self.__gh_list_query__()\n",
    "\n",
    "    def __gh_list_query__(self):\n",
    "        \"\"\"\n",
    "        Calls the GitHub API via the GitHub CLI (`gh run list`) and retrieves\n",
    "        a specified number of workflows.\n",
    "\n",
    "        :return: A DataFrame containing the parsed workflow data.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            list_command = f'gh run --repo {self.repository} list {self.json_attributes} -L {self.query_size}'\n",
    "            \n",
    "            output_json = subprocess.run(\n",
    "                list_command, shell=True, text=True, check=True, capture_output=True\n",
    "            ).stdout\n",
    "\n",
    "            parsed_json = ArqManipulation.parse_stdout_json(output_json)\n",
    "            df = ArqManipulation.json_to_df(parsed_json)\n",
    "\n",
    "            ArqManipulation.save_df_to_parquet(df = df, parquet_file_name=\"./bin/actionsWorflow.parquet\")\n",
    "\n",
    "            return df.set_index('name')\n",
    "\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error executing GitHub CLI command: {e}\")\n",
    "            return pd.DataFrame()  # Return an empty DataFrame on error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionsJobs:\n",
    "    \"\"\"\n",
    "    A class to interact with GitHub Actions jobs using the GitHub CLI.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, repository, workflow):\n",
    "        \"\"\"\n",
    "        Initializes the ActionsJobs class.\n",
    "\n",
    "        :param repository: GitHub repository in the format \"owner/repo\".\n",
    "        :param workflow: Workflow associated with the jobs.\n",
    "        \"\"\"\n",
    "        self.repository = repository\n",
    "        self.workflow = workflow  \n",
    "\n",
    "    def __retrieve_jobs__(self, database_id: int):\n",
    "        command = f'gh run --repo {self.repository} view {database_id}'\n",
    "        jobs_data = subprocess.run(command, shell=True, text=True, check=True, capture_output=True).stdout\n",
    "\n",
    "        return jobs_data\n",
    "\n",
    "    def get_jobs(self, database_id: int) -> pd.DataFrame:\n",
    "            \"\"\"\n",
    "            Retrieves job data from the GitHub CLI and processes it.\n",
    "\n",
    "            :param database_id: The ID of the workflow run.\n",
    "            :return: A Pandas DataFrame containing job details.\n",
    "            \"\"\"\n",
    "            try:\n",
    "                jobs_df = ArqManipulation.read_parquet_file(parquet_file_name=\"./bin/actionsJobs.parquet\")\n",
    "\n",
    "                if jobs_df.empty:\n",
    "                    data = self.__retrieve_jobs__(database_id=database_id)\n",
    "                    jobs_df = self.__clean_job_text__(data)\n",
    "\n",
    "                    jobs_df[\"databaseId\"] = int(database_id)\n",
    "\n",
    "                    ArqManipulation.save_df_to_parquet(jobs_df, parquet_file_name=\"./bin/actionsJobs.parquet\")\n",
    "\n",
    "                elif not database_id in jobs_df['databaseId'].values:\n",
    "                    data = self.__retrieve_jobs__(database_id=database_id)\n",
    "                    data_df = self.__clean_job_text__(data)\n",
    "                    data_df[\"databaseId\"] = int(database_id)\n",
    "\n",
    "                    jobs_df = pd.concat([jobs_df, data_df], ignore_index=True)\n",
    "\n",
    "                    ArqManipulation.save_df_to_parquet(jobs_df, parquet_file_name=\"./bin/actionsJobs.parquet\")\n",
    "\n",
    "                return jobs_df\n",
    "\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"Error executing GitHub CLI command: {e}\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error: {e}\")\n",
    "                return pd.DataFrame()\n",
    "        \n",
    "    def __split_string__(self, job_list):\n",
    "        \"\"\"\n",
    "        Splits a job string into structured components.\n",
    "\n",
    "        :param job: The job string to split.\n",
    "        :return: A list of cleaned job attributes.\n",
    "        \"\"\"\n",
    "        jobs = []\n",
    "\n",
    "        for job in job_list:\n",
    "            delimiters = r\" \\| | / build in | \\(ID |\\| in| / cleanup in | /| in \" \n",
    "            splitted_job = re.split(delimiters, job)\n",
    "            splitted_job = [s.strip() for s in splitted_job if s.strip()]\n",
    "            jobs.append(splitted_job)\n",
    "        \n",
    "        jobs.pop(0)\n",
    "\n",
    "        return jobs\n",
    "\n",
    "    def __build_cleaned_df__(self, data):\n",
    "        # Define columns\n",
    "        columns = [\"conclusion\", \"test\", \"buildTime (sec)\", \"jobId\"]\n",
    "        jobs_df = pd.DataFrame(columns=columns)\n",
    "        jobs_df[\"failedAt\"] = None\n",
    "\n",
    "        for job in data:\n",
    "            if any(\"ID\" in item and (\"PASSED\" in item or \"FAILED\" in item) for item in job):\n",
    "                temp_df = pd.DataFrame(self.__split_string__(job), columns=columns)\n",
    "\n",
    "                temp_df['buildTime (sec)'] = temp_df['buildTime (sec)'].apply(str_time_to_int)\n",
    "                jobs_df = pd.concat([jobs_df, temp_df], ignore_index=True)\n",
    "            \n",
    "            elif any(\"FAILED\" in item for item in job):\n",
    "                failed = next(item for item in job if \"FAILED\" in item).split(\"FAILED | \")\n",
    "                if not jobs_df.empty:\n",
    "                    jobs_df.at[jobs_df.index[-1], \"failedAt\"] = failed[1]  \n",
    "\n",
    "        jobs_df[\"jobId\"] = jobs_df[\"jobId\"].str.rstrip(\")\").astype('int')\n",
    "        return jobs_df\n",
    "\n",
    "\n",
    "    def __find_jobs__(self, base_str: str) -> list[str]:\n",
    "        lines = base_str.splitlines()\n",
    "        arr = []  # Stores grouped sections\n",
    "        current_group = []  # Temporary storage for the current section\n",
    "\n",
    "        for line in lines:\n",
    "            if line.isupper() or not line.strip():  # New section (uppercase or empty line)\n",
    "                if current_group:  # Avoid adding empty groups\n",
    "                    arr.append(current_group)\n",
    "                current_group = [line]  # Start a new group\n",
    "            else:\n",
    "                current_group.append(line)\n",
    "\n",
    "        if current_group:  # Append the last group\n",
    "            arr.append(current_group)\n",
    "\n",
    "        # Filter out groups that do not start with an uppercase title\n",
    "        filtered_arr = [group for group in arr if group and group[0].isupper()]\n",
    "        return filtered_arr\n",
    "\n",
    "    def __clean_job_text__(self, base_str: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Cleans and structures GitHub job data from CLI output.\n",
    "\n",
    "        :param base_str: Raw job text output from the GitHub CLI.\n",
    "        :return: A Pandas DataFrame with structured job data.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Remove ANSI escape sequences and unwanted characters\n",
    "            ansi_cleaned = ArqManipulation.clean_ansi_escape(base_str)\n",
    "            cleaned = ansi_cleaned.replace(\"✓\", \"PASSED |\").replace(\"X\", \"FAILED |\")\n",
    "\n",
    "            stripped_list = self.__find_jobs__(cleaned)\n",
    "\n",
    "            if not (x.find('JOBS') or x.find(\"ANNOTATIONS\") for x in stripped_list):\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            jobs_df = self.__build_cleaned_df__(stripped_list)\n",
    "\n",
    "            return jobs_df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing job text: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    \n",
    "\n",
    "def str_time_to_int(time_str: str) -> int:\n",
    "    \"\"\"\n",
    "    Converts a time string to seconds.\n",
    "    returns: int\n",
    "    \"\"\"\n",
    "    names = ['d', 'h', 'm', 's']\n",
    "    seconds = [86400, 3600, 60, 1]\n",
    "\n",
    "    total_time = 0\n",
    "\n",
    "    for m, t in zip(names,seconds):\n",
    "        if m in time_str:\n",
    "            time_list = time_str.split(m)\n",
    "            total_time +=  int(time_list[0]) * t\n",
    "            time_str = time_list[1]\n",
    "\n",
    "    return total_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionsArtifacts:\n",
    "    \"\"\"\n",
    "    A class to handle downloading, retrieving, and deleting GitHub Actions artifacts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, repository: str):\n",
    "        \"\"\"\n",
    "        Initializes the ActionsArtifacts object.\n",
    "\n",
    "        :param repository: The GitHub repository in the format \"owner/repo\".\n",
    "        \"\"\"\n",
    "        self.repository = repository\n",
    "        self.folder = './artifacts/'  # Default storage dir\n",
    "        self.paths = self.retrieve_downloaded_artifacts() \n",
    "\n",
    "    def download_artifact(self, database_id: str):\n",
    "        \"\"\"\n",
    "        Downloads an artifact from GitHub Actions using the GitHub CLI.\n",
    "\n",
    "        :param database_id: The database ID of the artifact to download.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Ensure the folder exists before downloading\n",
    "            os.makedirs(self.folder, exist_ok=True)\n",
    "\n",
    "            # Construct the command to download the artifact\n",
    "            command = f'gh run --repo {self.repository} download {database_id} --dir {os.path.join(self.folder, str(database_id))}'\n",
    "\n",
    "            # Execute the command\n",
    "            subprocess.run(command, shell=True, text=True, check=True)\n",
    "            print(\"Download Successful\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error during artifact download: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "\n",
    "    def retrieve_downloaded_artifacts(self) -> list[str]:\n",
    "        \"\"\"\n",
    "        Retrieves all downloaded artifacts file paths.\n",
    "\n",
    "        :return: returns Paths of the downloaded artifacts\n",
    "        \"\"\"\n",
    "        paths = []\n",
    "\n",
    "        # Walk through the artifacts folder and collect all file paths\n",
    "        for path, _, files in os.walk(self.folder):\n",
    "            for file in files:\n",
    "                paths.append(os.path.join(path, file))\n",
    "\n",
    "        return paths\n",
    "\n",
    "    def delete_downloaded_artifacts(self):\n",
    "        \"\"\"\n",
    "        Deletes all downloaded artifacts recursively\n",
    "        \"\"\"\n",
    "        try:\n",
    "            shutil.rmtree(self.folder)\n",
    "            if os.path.exists(self.folder):\n",
    "                print(\"Error: Failed to delete artifacts directory.\")\n",
    "            else:\n",
    "                print(\"Artifacts directory deleted successfully.\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"Artifacts directory not found, nothing to delete.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error while deleting artifacts: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PytestArtifactLogExtractor:\n",
    "    \"\"\"\n",
    "    A class to extract and process test status and timing information from a pytest artifact log.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path: str):\n",
    "        \"\"\"\n",
    "        Initializes the PytestArtifactLogExtractor object.\n",
    "\n",
    "        :param path: Path to the pytest artifact log file.\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        self.data = self.__read_file__()\n",
    "\n",
    "    def __read_file__(self):\n",
    "        \"\"\"\n",
    "        Reads the contents of the log file and returns it as a string.\n",
    "\n",
    "        :return: String containing the file content.\n",
    "        \"\"\"\n",
    "        with open(self.path, \"r\") as file: \n",
    "            data = file.read()\n",
    "\n",
    "        return ArqManipulation.clean_ansi_escape(data)\n",
    "\n",
    "    def log_to_df(self):\n",
    "        \"\"\"\n",
    "        Parses the log file to extract test results and performance metrics.\n",
    "\n",
    "        :return: A DataFrame combining test statuses with time metrics.\n",
    "        \"\"\"\n",
    "\n",
    "        df_parquet = ArqManipulation.read_parquet_file(parquet_file_name='pytest.log.parquet')   \n",
    "\n",
    "        databaseId = self.__extract_self_path_info__().get('databaseId').get(0)\n",
    "        databaseId = int(databaseId) if databaseId else None \n",
    "    \n",
    "        if not df_parquet.empty and (databaseId in df_parquet['databaseId']):\n",
    "            return df_parquet\n",
    "\n",
    "        tests, categories, failures = self.__extract_all_categories__()\n",
    "        \n",
    "        # Creating dataframes test status and categories\n",
    "        status_df = pd.DataFrame(tests, columns=[\"status\", \"name\", \"category\", \"arguments\"]).set_index('name')\n",
    "        categories_df = self.__create_df__(categories)\n",
    "        failures_df = pd.DataFrame(failures,columns=['name', 'category', 'arguments', 'error', 'error_details']).set_index('name')\n",
    "\n",
    "        # Labeling the dfs\n",
    "        status_df.index.name = 'pytest_tests_status'\n",
    "        categories_df.index.name = 'pytest_run_times'\n",
    "        failures_df.index.name = 'pytest_failures_errors'\n",
    "\n",
    "        # Applying individual id for each table\n",
    "        status_df['databaseId'] = databaseId\n",
    "        categories_df['databaseId'] = databaseId\n",
    "        failures_df['databaseId'] = databaseId\n",
    "\n",
    "\n",
    "        return status_df, categories_df, failures_df\n",
    "\n",
    "    def __extract_all_categories__(self):\n",
    "        \"\"\"\n",
    "        Converts extracted timing data into DataFrames.\n",
    "\n",
    "        :param values: A list of lists with extracted time metrics.\n",
    "        :type values: list[list]\n",
    "        :return: A list of DataFrames with execution time statistics.\n",
    "        :rtype: list[pandas.DataFrame]\n",
    "        \"\"\"\n",
    "        header = []\n",
    "        # Filtering out irrelevant categories\n",
    "        keywords = ('deselected', 'passed in', 'grand total', 'live log')\n",
    "\n",
    "        values = self.data.splitlines()\n",
    "        for value in values:\n",
    "            if any(k in value for k in keywords):\n",
    "                continue   \n",
    "            elif re.match(r'=+|-+', value): # Divide by headers demarked by '=' or '-' (logging)\n",
    "                value = value.replace(\"=\", \"\")  \n",
    "                value = value.replace(\"-\", \"\")  \n",
    "                header.append([value]) \n",
    "            else:\n",
    "                # Populate each category and break in the case of the pytest-durations tables while ignoring empty values\n",
    "                value = re.split(r\"\\s+\", value) \n",
    "                if list(filter(None, value)):\n",
    "                    header[-1].append(list(filter(None, value)))\n",
    "\n",
    "        headers = [['live_log','live_log','live_log']]\n",
    "        if not 'live log' in self.data:\n",
    "            headers = self.__extract_test_status_names__(self.__get_list_by_name__(header, 'session')[0])\n",
    "            \n",
    "        categories = self.__get_list_by_name__(header, 'duration top')\n",
    "        failures = self.__extract_failures_errors__(self.__get_list_by_name__(header, 'summary')[0])\n",
    "\n",
    "        return headers, categories, failures\n",
    "\n",
    "    def __get_list_by_name__(self, data: list, name: str):\n",
    "        \"\"\"\n",
    "        Find the sublist containing the specified name in the first element.\n",
    "\n",
    "        :param data: A list of sublists to search through.\n",
    "        :type data: list[list]\n",
    "        :param name: The name to search for in the first element of each sublist.\n",
    "        :type name: str\n",
    "        :return: A list of sublists where the first element matches the name.\n",
    "        :rtype: list[list]\n",
    "        \"\"\"\n",
    "        matching_sublists = []\n",
    "        \n",
    "        for sublist in data:\n",
    "            if re.search(name, sublist[0]):  # Converte os itens para string\n",
    "                matching_sublists.append(sublist)\n",
    "        \n",
    "        return matching_sublists\n",
    "\n",
    "    def __extract_test_status_names__(self, data):\n",
    "        \"\"\"\n",
    "        Extracts the status and the tests names out of the pytest log, breaking them down to a list of lists.\n",
    "\n",
    "        :param data: A list of lines containing test results.\n",
    "        :type data: list[str]\n",
    "        :return: list[list[str]]: A list of lists with test names, statuses (PASSED, FAILED, ERROR), and additional details.\n",
    "        \"\"\"\n",
    "\n",
    "        tests = []\n",
    "        keywords = ('PASSED', 'FAILED', 'ERROR')\n",
    "\n",
    "        for line in data:\n",
    "            line = ''.join(line).strip()\n",
    "            \n",
    "            if any(k in line for k in keywords):\n",
    "                line = re.sub(r'\\[.*?\\d%\\]', '', line)\n",
    "                parts = line.split('::', maxsplit=1)\n",
    "\n",
    "                match = re.search(r'(PASSED|FAILED|ERROR)', parts[0])\n",
    "                if match:\n",
    "                    test_name = parts[0][:match.start()].strip()\n",
    "                    status = match.group(0)\n",
    "                else:\n",
    "                    test_name, status = parts[0], None\n",
    "\n",
    "                tmp = [test_name, status]\n",
    "\n",
    "                if len(parts) > 1:\n",
    "                    values = list(filter(None, re.split(r'\\[(.*?)\\]', parts[1])))\n",
    "                    tmp += values\n",
    "                \n",
    "                while len(tmp) < 4:\n",
    "                    tmp.append(None)\n",
    "\n",
    "                tests.append(tmp)\n",
    "        \n",
    "        return tests\n",
    "\n",
    "    def __extract_failures_errors__(self, data):\n",
    "\n",
    "        \"\"\"\n",
    "        Extracts from the pytest log the details of tests with failures or errors cleaning the data to make it ready to a dataframe.\n",
    "\n",
    "        :param data: A list of strings containing test results.\n",
    "        :type data: list[str]\n",
    "        :return: list[list]: A list of lists containing details of tests with failures and/or errors.\n",
    "        \"\"\"\n",
    "\n",
    "        # Regex asks for a string, cleaning it and concatening the list\n",
    "        data_str = ''.join(list(''.join(d) for d in data[1:]))\n",
    "        data_str = list(filter(None, re.split(r'(FAILED|ERROR)', data_str)))\n",
    "\n",
    "        splitted_data = []\n",
    "\n",
    "        # Splitting test from error\n",
    "        for d in data_str:\n",
    "            if d and ('FAILED' or 'ERROR') not in d:\n",
    "                splitted_data.append(list(filter(None, re.split(r'\\[(.*?)\\]-|::|(\\w+):([\\w=*]+)', d))))   \n",
    "\n",
    "        return splitted_data\n",
    "\n",
    "    def __create_df__(self, values):\n",
    "        \"\"\"\n",
    "        Converts extracted timing information into DataFrames.\n",
    "\n",
    "        :param values: A list of lists containing extracted time metrics.\n",
    "        :return: A list of DataFrames with execution time statistics.\n",
    "        \"\"\"\n",
    "        dfs = pd.DataFrame()\n",
    "        \n",
    "        for h in values:\n",
    "            time_df = pd.DataFrame(h[2:], columns=h[1])\n",
    "\n",
    "            # Converting time-related columns to datetime.time format\n",
    "            time_columns = ['avg', 'min', 'total']\n",
    "            for col in time_columns:\n",
    "                if col in time_df.columns:\n",
    "                    time_df[col] = pd.to_datetime(time_df[col], format=\"%H:%M:%S.%f\", errors='coerce').dt.time  \n",
    "\n",
    "            # Assigning a 'durationType' column for metric categorization\n",
    "            time_df['durationType'] = h[0].replace('top', '').replace('test', '')\n",
    "\n",
    "            dfs = pd.concat([time_df, dfs], ignore_index=True)\n",
    "\n",
    "        if 'name' in dfs.columns:\n",
    "            dfs = dfs.set_index('name') \n",
    "\n",
    "        return dfs\n",
    "\n",
    "    def __extract_self_path_info__(self):\n",
    "        \"\"\"\n",
    "        Extracts test and database ID information from the log file path.\n",
    "\n",
    "        :return: A DataFrame containing 'test' and 'databaseId' information.\n",
    "        \"\"\"\n",
    "        # Extract filename without extension\n",
    "        stripped = self.path.split('/')[-1].split('.')\n",
    "        stripped.pop()  # Remove the file extension\n",
    "\n",
    "        # Ensure there are exactly three elements (fill missing ones with None)\n",
    "        while len(stripped) < 3:\n",
    "            stripped.append(None)  # Fill missing values with NaN\n",
    "\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame([stripped], columns=['test', 'region', 'databaseId'])\n",
    "\n",
    "        return df\n",
    "\n",
    "    def __merge_artifact_dfs__(self, times_df, status_df):\n",
    "        \"\"\"\n",
    "        Merges test execution time data with test status information.\n",
    "\n",
    "        :param times_df: A list of DataFrames containing time-related data.\n",
    "        :param status_df: A DataFrame containing test statuses.\n",
    "        :return: A combined DataFrame containing execution metrics and test results.\n",
    "        \"\"\"\n",
    "        databaseId_df = self.__extract_self_path_info__()  \n",
    "        order = ['category', 'durationType', 'databaseId', 'status', 'num', 'avg', 'min', 'total']\n",
    "        dfs = []\n",
    "\n",
    "        for h in times_df:\n",
    "            joined_df = h.join(status_df)  # Merging time metrics with test statuses\n",
    "\n",
    "            # Adding database ID to each row\n",
    "            for col in databaseId_df.columns.values:\n",
    "                joined_df[col] = databaseId_df[col].values[0]  \n",
    "\n",
    "            # Reordering columns\n",
    "            joined_df = joined_df[order]  \n",
    "            dfs.append(joined_df)\n",
    "\n",
    "        return pd.concat(dfs)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conclusion</th>\n",
       "      <th>test</th>\n",
       "      <th>buildTime (sec)</th>\n",
       "      <th>jobId</th>\n",
       "      <th>failedAt</th>\n",
       "      <th>databaseId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>run_tests (cold_storage, cold_storage, ../para...</td>\n",
       "      <td>85</td>\n",
       "      <td>37044161130</td>\n",
       "      <td>None</td>\n",
       "      <td>13269149127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>run_tests (basic, basic, ../params.example.yam...</td>\n",
       "      <td>96</td>\n",
       "      <td>37044161606</td>\n",
       "      <td>None</td>\n",
       "      <td>13269149127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>run_tests (presign, presign, ../params.example...</td>\n",
       "      <td>69</td>\n",
       "      <td>37044161918</td>\n",
       "      <td>None</td>\n",
       "      <td>13269149127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>tests-success</td>\n",
       "      <td>0</td>\n",
       "      <td>37044262983</td>\n",
       "      <td>None</td>\n",
       "      <td>13269149127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>cleanup-tests</td>\n",
       "      <td>102</td>\n",
       "      <td>37044273275</td>\n",
       "      <td>None</td>\n",
       "      <td>13269149127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>extra_tests_debug (locking, ../params/br-ne1.y...</td>\n",
       "      <td>222</td>\n",
       "      <td>37031740446</td>\n",
       "      <td>None</td>\n",
       "      <td>13265481700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>extra_tests_debug (locking, ../params/br-se1.y...</td>\n",
       "      <td>10945</td>\n",
       "      <td>37031741305</td>\n",
       "      <td>None</td>\n",
       "      <td>13265481700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>extra_tests_dist (bucket_versioning, ../params...</td>\n",
       "      <td>101</td>\n",
       "      <td>37031741997</td>\n",
       "      <td>None</td>\n",
       "      <td>13265481700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>extra_tests_dist (acl, ../params/br-ne1.yaml, ...</td>\n",
       "      <td>323</td>\n",
       "      <td>37031742597</td>\n",
       "      <td>None</td>\n",
       "      <td>13265481700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>extra_tests_dist (policy, ../params/br-ne1.yam...</td>\n",
       "      <td>156</td>\n",
       "      <td>37031743174</td>\n",
       "      <td>None</td>\n",
       "      <td>13265481700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>cleanup_tests</td>\n",
       "      <td>105</td>\n",
       "      <td>37044113674</td>\n",
       "      <td>None</td>\n",
       "      <td>13265481700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>extra_tests_debug (locking, ../params/br-se1.y...</td>\n",
       "      <td>2168</td>\n",
       "      <td>37043704535</td>\n",
       "      <td>None</td>\n",
       "      <td>13269014124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>extra_tests_debug (locking, ../params/br-ne1.y...</td>\n",
       "      <td>246</td>\n",
       "      <td>37043705230</td>\n",
       "      <td>None</td>\n",
       "      <td>13269014124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>extra_tests_dist (acl, ../params/br-ne1.yaml, ...</td>\n",
       "      <td>398</td>\n",
       "      <td>37043705967</td>\n",
       "      <td>None</td>\n",
       "      <td>13269014124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>extra_tests_dist (bucket_versioning, ../params...</td>\n",
       "      <td>118</td>\n",
       "      <td>37043706472</td>\n",
       "      <td>None</td>\n",
       "      <td>13269014124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>extra_tests_dist (policy, ../params/br-ne1.yam...</td>\n",
       "      <td>165</td>\n",
       "      <td>37043706867</td>\n",
       "      <td>None</td>\n",
       "      <td>13269014124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>cleanup_tests</td>\n",
       "      <td>113</td>\n",
       "      <td>37045803570</td>\n",
       "      <td>None</td>\n",
       "      <td>13269014124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   conclusion                                               test  \\\n",
       "0      PASSED  run_tests (cold_storage, cold_storage, ../para...   \n",
       "1      PASSED  run_tests (basic, basic, ../params.example.yam...   \n",
       "2      PASSED  run_tests (presign, presign, ../params.example...   \n",
       "3      PASSED                                      tests-success   \n",
       "4      PASSED                                      cleanup-tests   \n",
       "5      PASSED  extra_tests_debug (locking, ../params/br-ne1.y...   \n",
       "6      PASSED  extra_tests_debug (locking, ../params/br-se1.y...   \n",
       "7      PASSED  extra_tests_dist (bucket_versioning, ../params...   \n",
       "8      PASSED  extra_tests_dist (acl, ../params/br-ne1.yaml, ...   \n",
       "9      PASSED  extra_tests_dist (policy, ../params/br-ne1.yam...   \n",
       "10     PASSED                                      cleanup_tests   \n",
       "11     PASSED  extra_tests_debug (locking, ../params/br-se1.y...   \n",
       "12     PASSED  extra_tests_debug (locking, ../params/br-ne1.y...   \n",
       "13     PASSED  extra_tests_dist (acl, ../params/br-ne1.yaml, ...   \n",
       "14     PASSED  extra_tests_dist (bucket_versioning, ../params...   \n",
       "15     PASSED  extra_tests_dist (policy, ../params/br-ne1.yam...   \n",
       "16     PASSED                                      cleanup_tests   \n",
       "\n",
       "    buildTime (sec)        jobId failedAt   databaseId  \n",
       "0                85  37044161130     None  13269149127  \n",
       "1                96  37044161606     None  13269149127  \n",
       "2                69  37044161918     None  13269149127  \n",
       "3                 0  37044262983     None  13269149127  \n",
       "4               102  37044273275     None  13269149127  \n",
       "5               222  37031740446     None  13265481700  \n",
       "6             10945  37031741305     None  13265481700  \n",
       "7               101  37031741997     None  13265481700  \n",
       "8               323  37031742597     None  13265481700  \n",
       "9               156  37031743174     None  13265481700  \n",
       "10              105  37044113674     None  13265481700  \n",
       "11             2168  37043704535     None  13269014124  \n",
       "12              246  37043705230     None  13269014124  \n",
       "13              398  37043705967     None  13269014124  \n",
       "14              118  37043706472     None  13269014124  \n",
       "15              165  37043706867     None  13269014124  \n",
       "16              113  37045803570     None  13269014124  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jobs = ActionsJobs(repo_path, workflow)\n",
    "jobs.get_jobs(13269014124)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Main\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame successfully saved to ./bin/actionsWorflow.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>createdAt</th>\n",
       "      <th>conclusion</th>\n",
       "      <th>status</th>\n",
       "      <th>databaseId</th>\n",
       "      <th>workflowDatabaseId</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pull Request Essential Tests</th>\n",
       "      <td>2025-02-11 14:42:01+00:00</td>\n",
       "      <td>success</td>\n",
       "      <td>completed</td>\n",
       "      <td>13265481705</td>\n",
       "      <td>132962917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pull Request Extra Tests</th>\n",
       "      <td>2025-02-11 14:42:01+00:00</td>\n",
       "      <td>success</td>\n",
       "      <td>completed</td>\n",
       "      <td>13265481700</td>\n",
       "      <td>142271933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pull Request Extra Tests</th>\n",
       "      <td>2025-02-11 17:37:25+00:00</td>\n",
       "      <td>success</td>\n",
       "      <td>completed</td>\n",
       "      <td>13269014124</td>\n",
       "      <td>142271933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pull Request Essential Tests</th>\n",
       "      <td>2025-02-11 17:37:25+00:00</td>\n",
       "      <td>success</td>\n",
       "      <td>completed</td>\n",
       "      <td>13269014120</td>\n",
       "      <td>132962917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pull Request Essential Tests</th>\n",
       "      <td>2025-02-11 17:39:58+00:00</td>\n",
       "      <td>success</td>\n",
       "      <td>completed</td>\n",
       "      <td>13269057739</td>\n",
       "      <td>132962917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pull Request Extra Tests</th>\n",
       "      <td>2025-02-11 17:39:58+00:00</td>\n",
       "      <td>success</td>\n",
       "      <td>completed</td>\n",
       "      <td>13269057738</td>\n",
       "      <td>142271933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pull Request Extra Tests</th>\n",
       "      <td>2025-02-11 17:45:26+00:00</td>\n",
       "      <td>success</td>\n",
       "      <td>completed</td>\n",
       "      <td>13269149128</td>\n",
       "      <td>142271933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pull Request Essential Tests</th>\n",
       "      <td>2025-02-11 17:45:26+00:00</td>\n",
       "      <td>success</td>\n",
       "      <td>completed</td>\n",
       "      <td>13269149127</td>\n",
       "      <td>132962917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pull Request Essential Tests</th>\n",
       "      <td>2025-02-11 17:52:13+00:00</td>\n",
       "      <td>success</td>\n",
       "      <td>completed</td>\n",
       "      <td>13269265728</td>\n",
       "      <td>132962917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pull Request Extra Tests</th>\n",
       "      <td>2025-02-11 17:52:13+00:00</td>\n",
       "      <td>success</td>\n",
       "      <td>completed</td>\n",
       "      <td>13269265723</td>\n",
       "      <td>142271933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             createdAt conclusion     status  \\\n",
       "name                                                                           \n",
       "Pull Request Essential Tests 2025-02-11 14:42:01+00:00    success  completed   \n",
       "Pull Request Extra Tests     2025-02-11 14:42:01+00:00    success  completed   \n",
       "Pull Request Extra Tests     2025-02-11 17:37:25+00:00    success  completed   \n",
       "Pull Request Essential Tests 2025-02-11 17:37:25+00:00    success  completed   \n",
       "Pull Request Essential Tests 2025-02-11 17:39:58+00:00    success  completed   \n",
       "Pull Request Extra Tests     2025-02-11 17:39:58+00:00    success  completed   \n",
       "Pull Request Extra Tests     2025-02-11 17:45:26+00:00    success  completed   \n",
       "Pull Request Essential Tests 2025-02-11 17:45:26+00:00    success  completed   \n",
       "Pull Request Essential Tests 2025-02-11 17:52:13+00:00    success  completed   \n",
       "Pull Request Extra Tests     2025-02-11 17:52:13+00:00    success  completed   \n",
       "\n",
       "                               databaseId  workflowDatabaseId  \n",
       "name                                                           \n",
       "Pull Request Essential Tests  13265481705           132962917  \n",
       "Pull Request Extra Tests      13265481700           142271933  \n",
       "Pull Request Extra Tests      13269014124           142271933  \n",
       "Pull Request Essential Tests  13269014120           132962917  \n",
       "Pull Request Essential Tests  13269057739           132962917  \n",
       "Pull Request Extra Tests      13269057738           142271933  \n",
       "Pull Request Extra Tests      13269149128           142271933  \n",
       "Pull Request Essential Tests  13269149127           132962917  \n",
       "Pull Request Essential Tests  13269265728           132962917  \n",
       "Pull Request Extra Tests      13269265723           142271933  "
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_path = 'MagaluCloud/s3-specs'\n",
    "query_size = 10\n",
    "\n",
    "workflow = ActionsWorkflow(repository=repo_path, query_size=query_size)\n",
    "workflow.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conclusion</th>\n",
       "      <th>test</th>\n",
       "      <th>buildTime (sec)</th>\n",
       "      <th>jobId</th>\n",
       "      <th>failedAt</th>\n",
       "      <th>databaseId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>run_tests (cold_storage, cold_storage, ../para...</td>\n",
       "      <td>85</td>\n",
       "      <td>37044161130</td>\n",
       "      <td>None</td>\n",
       "      <td>13269149127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>run_tests (basic, basic, ../params.example.yam...</td>\n",
       "      <td>96</td>\n",
       "      <td>37044161606</td>\n",
       "      <td>None</td>\n",
       "      <td>13269149127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>run_tests (presign, presign, ../params.example...</td>\n",
       "      <td>69</td>\n",
       "      <td>37044161918</td>\n",
       "      <td>None</td>\n",
       "      <td>13269149127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>tests-success</td>\n",
       "      <td>0</td>\n",
       "      <td>37044262983</td>\n",
       "      <td>None</td>\n",
       "      <td>13269149127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>cleanup-tests</td>\n",
       "      <td>102</td>\n",
       "      <td>37044273275</td>\n",
       "      <td>None</td>\n",
       "      <td>13269149127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>extra_tests_debug (locking, ../params/br-ne1.y...</td>\n",
       "      <td>222</td>\n",
       "      <td>37031740446</td>\n",
       "      <td>None</td>\n",
       "      <td>13265481700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>extra_tests_debug (locking, ../params/br-se1.y...</td>\n",
       "      <td>10945</td>\n",
       "      <td>37031741305</td>\n",
       "      <td>None</td>\n",
       "      <td>13265481700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>extra_tests_dist (bucket_versioning, ../params...</td>\n",
       "      <td>101</td>\n",
       "      <td>37031741997</td>\n",
       "      <td>None</td>\n",
       "      <td>13265481700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>extra_tests_dist (acl, ../params/br-ne1.yaml, ...</td>\n",
       "      <td>323</td>\n",
       "      <td>37031742597</td>\n",
       "      <td>None</td>\n",
       "      <td>13265481700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>extra_tests_dist (policy, ../params/br-ne1.yam...</td>\n",
       "      <td>156</td>\n",
       "      <td>37031743174</td>\n",
       "      <td>None</td>\n",
       "      <td>13265481700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>cleanup_tests</td>\n",
       "      <td>105</td>\n",
       "      <td>37044113674</td>\n",
       "      <td>None</td>\n",
       "      <td>13265481700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>extra_tests_debug (locking, ../params/br-se1.y...</td>\n",
       "      <td>2168</td>\n",
       "      <td>37043704535</td>\n",
       "      <td>None</td>\n",
       "      <td>13269014124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>extra_tests_debug (locking, ../params/br-ne1.y...</td>\n",
       "      <td>246</td>\n",
       "      <td>37043705230</td>\n",
       "      <td>None</td>\n",
       "      <td>13269014124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>extra_tests_dist (acl, ../params/br-ne1.yaml, ...</td>\n",
       "      <td>398</td>\n",
       "      <td>37043705967</td>\n",
       "      <td>None</td>\n",
       "      <td>13269014124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>extra_tests_dist (bucket_versioning, ../params...</td>\n",
       "      <td>118</td>\n",
       "      <td>37043706472</td>\n",
       "      <td>None</td>\n",
       "      <td>13269014124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>extra_tests_dist (policy, ../params/br-ne1.yam...</td>\n",
       "      <td>165</td>\n",
       "      <td>37043706867</td>\n",
       "      <td>None</td>\n",
       "      <td>13269014124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>PASSED</td>\n",
       "      <td>cleanup_tests</td>\n",
       "      <td>113</td>\n",
       "      <td>37045803570</td>\n",
       "      <td>None</td>\n",
       "      <td>13269014124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   conclusion                                               test  \\\n",
       "0      PASSED  run_tests (cold_storage, cold_storage, ../para...   \n",
       "1      PASSED  run_tests (basic, basic, ../params.example.yam...   \n",
       "2      PASSED  run_tests (presign, presign, ../params.example...   \n",
       "3      PASSED                                      tests-success   \n",
       "4      PASSED                                      cleanup-tests   \n",
       "5      PASSED  extra_tests_debug (locking, ../params/br-ne1.y...   \n",
       "6      PASSED  extra_tests_debug (locking, ../params/br-se1.y...   \n",
       "7      PASSED  extra_tests_dist (bucket_versioning, ../params...   \n",
       "8      PASSED  extra_tests_dist (acl, ../params/br-ne1.yaml, ...   \n",
       "9      PASSED  extra_tests_dist (policy, ../params/br-ne1.yam...   \n",
       "10     PASSED                                      cleanup_tests   \n",
       "11     PASSED  extra_tests_debug (locking, ../params/br-se1.y...   \n",
       "12     PASSED  extra_tests_debug (locking, ../params/br-ne1.y...   \n",
       "13     PASSED  extra_tests_dist (acl, ../params/br-ne1.yaml, ...   \n",
       "14     PASSED  extra_tests_dist (bucket_versioning, ../params...   \n",
       "15     PASSED  extra_tests_dist (policy, ../params/br-ne1.yam...   \n",
       "16     PASSED                                      cleanup_tests   \n",
       "\n",
       "    buildTime (sec)        jobId failedAt   databaseId  \n",
       "0                85  37044161130     None  13269149127  \n",
       "1                96  37044161606     None  13269149127  \n",
       "2                69  37044161918     None  13269149127  \n",
       "3                 0  37044262983     None  13269149127  \n",
       "4               102  37044273275     None  13269149127  \n",
       "5               222  37031740446     None  13265481700  \n",
       "6             10945  37031741305     None  13265481700  \n",
       "7               101  37031741997     None  13265481700  \n",
       "8               323  37031742597     None  13265481700  \n",
       "9               156  37031743174     None  13265481700  \n",
       "10              105  37044113674     None  13265481700  \n",
       "11             2168  37043704535     None  13269014124  \n",
       "12              246  37043705230     None  13269014124  \n",
       "13              398  37043705967     None  13269014124  \n",
       "14              118  37043706472     None  13269014124  \n",
       "15              165  37043706867     None  13269014124  \n",
       "16              113  37045803570     None  13269014124  "
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs = ActionsJobs(repo_path, workflow)\n",
    "jobs.get_jobs(13269014124)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./artifacts/local_artifact.br_se1.123456.log',\n",
       " './artifacts/13160019050/output_artifact_not_cli_and_locking_se1.13160019050/pytest_output_not_cli_and_locking_se1.13160019050.log',\n",
       " './artifacts/13160019050/output_artifact_not_cli_and_locking_ne1.13160019050/pytest_output_not_cli_and_locking_ne1.13160019050.log',\n",
       " './artifacts/13269014124/output_artifact_policy_br.ne1.13269014124.13269014124/pytest_output_policy_br_ne1.13269014124.log',\n",
       " './artifacts/13269014124/output_artifact_locking_br.ne1.13269014124.13269014124/pytest_output_locking_br.ne1.13269014124.log',\n",
       " './artifacts/13269014124/output_artifact_bucket_versioning_br.ne1.13269014124.13269014124/pytest_output_bucket_versioning.br_ne1.13269014124.log',\n",
       " './artifacts/13269014124/output_artifact_locking_br.se1.13269014124.13269014124/pytest_output_locking_br_se1.13269014124.log',\n",
       " './artifacts/13269014124/output_artifact_acl_br.ne1.13269014124.13269014124/pytest_output_acl.br_ne1.13269014124.log']"
      ]
     },
     "execution_count": 1263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artifacts = ActionsArtifacts(repository=repo_path)\n",
    "#a = artifacts.download_artifact(13269014124)\n",
    "a = artifacts.retrieve_downloaded_artifacts()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1701,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'pytest.log.parquet' does not exist.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>category</th>\n",
       "      <th>arguments</th>\n",
       "      <th>databaseId</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pytest_tests_status</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FAILED</th>\n",
       "      <td></td>\n",
       "      <td>test_upload_multiple_objects</td>\n",
       "      <td>num=100-5-10</td>\n",
       "      <td>123456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PASSED</th>\n",
       "      <td></td>\n",
       "      <td>test_upload_multiple_objects</td>\n",
       "      <td>num=100-3-10</td>\n",
       "      <td>123456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FAILED</th>\n",
       "      <td></td>\n",
       "      <td>test_upload_multiple_objects</td>\n",
       "      <td>num=100-9-10</td>\n",
       "      <td>123456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PASSED</th>\n",
       "      <td></td>\n",
       "      <td>test_upload_multiple_objects</td>\n",
       "      <td>num=100-1-10</td>\n",
       "      <td>123456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PASSED</th>\n",
       "      <td></td>\n",
       "      <td>test_upload_multiple_objects</td>\n",
       "      <td>num=100-7-10</td>\n",
       "      <td>123456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PASSED</th>\n",
       "      <td></td>\n",
       "      <td>test_upload_multiple_objects</td>\n",
       "      <td>num=100-6-10</td>\n",
       "      <td>123456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PASSED</th>\n",
       "      <td></td>\n",
       "      <td>test_upload_multiple_objects</td>\n",
       "      <td>num=100-4-10</td>\n",
       "      <td>123456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PASSED</th>\n",
       "      <td></td>\n",
       "      <td>test_download_multiple_objects</td>\n",
       "      <td>num=100-1-10</td>\n",
       "      <td>123456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PASSED</th>\n",
       "      <td></td>\n",
       "      <td>test_upload_multiple_objects</td>\n",
       "      <td>num=100-10-10</td>\n",
       "      <td>123456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PASSED</th>\n",
       "      <td></td>\n",
       "      <td>test_download_multiple_objects</td>\n",
       "      <td>num=100-5-10</td>\n",
       "      <td>123456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FAILED</th>\n",
       "      <td></td>\n",
       "      <td>test_download_multiple_objects</td>\n",
       "      <td>num=100-3-10</td>\n",
       "      <td>123456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PASSED</th>\n",
       "      <td></td>\n",
       "      <td>test_download_multiple_objects</td>\n",
       "      <td>num=100-9-10</td>\n",
       "      <td>123456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PASSED</th>\n",
       "      <td></td>\n",
       "      <td>test_download_multiple_objects</td>\n",
       "      <td>num=100-7-10</td>\n",
       "      <td>123456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PASSED</th>\n",
       "      <td></td>\n",
       "      <td>test_upload_multiple_objects</td>\n",
       "      <td>num=100-8-10</td>\n",
       "      <td>123456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PASSED</th>\n",
       "      <td></td>\n",
       "      <td>test_upload_multiple_objects</td>\n",
       "      <td>num=100-2-10</td>\n",
       "      <td>123456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PASSED</th>\n",
       "      <td></td>\n",
       "      <td>test_download_multiple_objects</td>\n",
       "      <td>num=100-2-10</td>\n",
       "      <td>123456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PASSED</th>\n",
       "      <td></td>\n",
       "      <td>test_download_multiple_objects</td>\n",
       "      <td>num=100-10-10</td>\n",
       "      <td>123456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FAILED</th>\n",
       "      <td></td>\n",
       "      <td>test_download_multiple_objects</td>\n",
       "      <td>num=100-4-10</td>\n",
       "      <td>123456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PASSED</th>\n",
       "      <td></td>\n",
       "      <td>test_download_multiple_objects</td>\n",
       "      <td>num=100-8-10</td>\n",
       "      <td>123456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PASSED</th>\n",
       "      <td></td>\n",
       "      <td>test_download_multiple_objects</td>\n",
       "      <td>num=100-6-10</td>\n",
       "      <td>123456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    status                        category      arguments  \\\n",
       "pytest_tests_status                                                         \n",
       "FAILED                        test_upload_multiple_objects   num=100-5-10   \n",
       "PASSED                        test_upload_multiple_objects   num=100-3-10   \n",
       "FAILED                        test_upload_multiple_objects   num=100-9-10   \n",
       "PASSED                        test_upload_multiple_objects   num=100-1-10   \n",
       "PASSED                        test_upload_multiple_objects   num=100-7-10   \n",
       "PASSED                        test_upload_multiple_objects   num=100-6-10   \n",
       "PASSED                        test_upload_multiple_objects   num=100-4-10   \n",
       "PASSED                      test_download_multiple_objects   num=100-1-10   \n",
       "PASSED                        test_upload_multiple_objects  num=100-10-10   \n",
       "PASSED                      test_download_multiple_objects   num=100-5-10   \n",
       "FAILED                      test_download_multiple_objects   num=100-3-10   \n",
       "PASSED                      test_download_multiple_objects   num=100-9-10   \n",
       "PASSED                      test_download_multiple_objects   num=100-7-10   \n",
       "PASSED                        test_upload_multiple_objects   num=100-8-10   \n",
       "PASSED                        test_upload_multiple_objects   num=100-2-10   \n",
       "PASSED                      test_download_multiple_objects   num=100-2-10   \n",
       "PASSED                      test_download_multiple_objects  num=100-10-10   \n",
       "FAILED                      test_download_multiple_objects   num=100-4-10   \n",
       "PASSED                      test_download_multiple_objects   num=100-8-10   \n",
       "PASSED                      test_download_multiple_objects   num=100-6-10   \n",
       "\n",
       "                     databaseId  \n",
       "pytest_tests_status              \n",
       "FAILED                   123456  \n",
       "PASSED                   123456  \n",
       "FAILED                   123456  \n",
       "PASSED                   123456  \n",
       "PASSED                   123456  \n",
       "PASSED                   123456  \n",
       "PASSED                   123456  \n",
       "PASSED                   123456  \n",
       "PASSED                   123456  \n",
       "PASSED                   123456  \n",
       "FAILED                   123456  \n",
       "PASSED                   123456  \n",
       "PASSED                   123456  \n",
       "PASSED                   123456  \n",
       "PASSED                   123456  \n",
       "PASSED                   123456  \n",
       "PASSED                   123456  \n",
       "FAILED                   123456  \n",
       "PASSED                   123456  \n",
       "PASSED                   123456  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>num</th>\n",
       "      <th>avg</th>\n",
       "      <th>min</th>\n",
       "      <th>durationType</th>\n",
       "      <th>databaseId</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pytest_run_times</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test_download_multiple_objects</th>\n",
       "      <td>00:00:40.494736</td>\n",
       "      <td>10</td>\n",
       "      <td>00:00:03.311439</td>\n",
       "      <td>00:00:02.036198</td>\n",
       "      <td>teardown duration</td>\n",
       "      <td>123456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_upload_multiple_objects</th>\n",
       "      <td>00:00:28.457474</td>\n",
       "      <td>10</td>\n",
       "      <td>00:00:02.639308</td>\n",
       "      <td>00:00:01.722496</td>\n",
       "      <td>teardown duration</td>\n",
       "      <td>123456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_download_multiple_objects</th>\n",
       "      <td>00:01:06.248248</td>\n",
       "      <td>10</td>\n",
       "      <td>00:00:06.635785</td>\n",
       "      <td>00:00:03.790442</td>\n",
       "      <td>setup duration</td>\n",
       "      <td>123456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_upload_multiple_objects</th>\n",
       "      <td>00:00:08.536476</td>\n",
       "      <td>10</td>\n",
       "      <td>00:00:00.840663</td>\n",
       "      <td>00:00:00.541395</td>\n",
       "      <td>setup duration</td>\n",
       "      <td>123456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_download_multiple_objects</th>\n",
       "      <td>00:01:05.466381</td>\n",
       "      <td>10</td>\n",
       "      <td>00:00:06.557192</td>\n",
       "      <td>00:00:05.550107</td>\n",
       "      <td>call duration</td>\n",
       "      <td>123456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_upload_multiple_objects</th>\n",
       "      <td>00:00:43.853014</td>\n",
       "      <td>10</td>\n",
       "      <td>00:00:04.014908</td>\n",
       "      <td>00:00:01.980053</td>\n",
       "      <td>call duration</td>\n",
       "      <td>123456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixture_upload_multiple_objects</th>\n",
       "      <td>00:00:57.071949</td>\n",
       "      <td>10</td>\n",
       "      <td>00:00:05.958852</td>\n",
       "      <td>00:00:03.087084</td>\n",
       "      <td>fixture duration</td>\n",
       "      <td>123456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixture_bucket_with_name</th>\n",
       "      <td>00:00:15.654320</td>\n",
       "      <td>20</td>\n",
       "      <td>00:00:00.843605</td>\n",
       "      <td>00:00:00.478260</td>\n",
       "      <td>fixture duration</td>\n",
       "      <td>123456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s3_client</th>\n",
       "      <td>00:00:02.010599</td>\n",
       "      <td>20</td>\n",
       "      <td>00:00:00.086407</td>\n",
       "      <td>00:00:00.056561</td>\n",
       "      <td>fixture duration</td>\n",
       "      <td>123456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_params</th>\n",
       "      <td>00:00:00.029676</td>\n",
       "      <td>20</td>\n",
       "      <td>00:00:00.001610</td>\n",
       "      <td>00:00:00.000740</td>\n",
       "      <td>fixture duration</td>\n",
       "      <td>123456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           total num              avg  \\\n",
       "pytest_run_times                                                        \n",
       "test_download_multiple_objects   00:00:40.494736  10  00:00:03.311439   \n",
       "test_upload_multiple_objects     00:00:28.457474  10  00:00:02.639308   \n",
       "test_download_multiple_objects   00:01:06.248248  10  00:00:06.635785   \n",
       "test_upload_multiple_objects     00:00:08.536476  10  00:00:00.840663   \n",
       "test_download_multiple_objects   00:01:05.466381  10  00:00:06.557192   \n",
       "test_upload_multiple_objects     00:00:43.853014  10  00:00:04.014908   \n",
       "fixture_upload_multiple_objects  00:00:57.071949  10  00:00:05.958852   \n",
       "fixture_bucket_with_name         00:00:15.654320  20  00:00:00.843605   \n",
       "s3_client                        00:00:02.010599  20  00:00:00.086407   \n",
       "test_params                      00:00:00.029676  20  00:00:00.001610   \n",
       "\n",
       "                                             min           durationType  \\\n",
       "pytest_run_times                                                          \n",
       "test_download_multiple_objects   00:00:02.036198    teardown duration     \n",
       "test_upload_multiple_objects     00:00:01.722496    teardown duration     \n",
       "test_download_multiple_objects   00:00:03.790442       setup duration     \n",
       "test_upload_multiple_objects     00:00:00.541395       setup duration     \n",
       "test_download_multiple_objects   00:00:05.550107        call duration     \n",
       "test_upload_multiple_objects     00:00:01.980053        call duration     \n",
       "fixture_upload_multiple_objects  00:00:03.087084     fixture duration     \n",
       "fixture_bucket_with_name         00:00:00.478260     fixture duration     \n",
       "s3_client                        00:00:00.056561     fixture duration     \n",
       "test_params                      00:00:00.000740     fixture duration     \n",
       "\n",
       "                                 databaseId  \n",
       "pytest_run_times                             \n",
       "test_download_multiple_objects       123456  \n",
       "test_upload_multiple_objects         123456  \n",
       "test_download_multiple_objects       123456  \n",
       "test_upload_multiple_objects         123456  \n",
       "test_download_multiple_objects       123456  \n",
       "test_upload_multiple_objects         123456  \n",
       "fixture_upload_multiple_objects      123456  \n",
       "fixture_bucket_with_name             123456  \n",
       "s3_client                            123456  \n",
       "test_params                          123456  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>arguments</th>\n",
       "      <th>error</th>\n",
       "      <th>error_details</th>\n",
       "      <th>databaseId</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pytest_failures_errors</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>docs/multiple_objects_test.py</th>\n",
       "      <td>test_upload_multiple_objects</td>\n",
       "      <td>num=100-5-10</td>\n",
       "      <td>AssertionError</td>\n",
       "      <td>Expectsuploads100tobeequaltoobjectsinthebucket...</td>\n",
       "      <td>123456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>docs/multiple_objects_test.py</th>\n",
       "      <td>test_upload_multiple_objects</td>\n",
       "      <td>num=100-9-10</td>\n",
       "      <td>AssertionError</td>\n",
       "      <td>Expectsuploads100tobeequaltoobjectsinthebucket...</td>\n",
       "      <td>123456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>docs/multiple_objects_test.py</th>\n",
       "      <td>test_download_multiple_objects</td>\n",
       "      <td>num=100-3-10</td>\n",
       "      <td>AssertionError</td>\n",
       "      <td>Expectsdownloads99tobeequaltouploads100assert9...</td>\n",
       "      <td>123456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>docs/multiple_objects_test.py</th>\n",
       "      <td>test_download_multiple_objects</td>\n",
       "      <td>num=100-4-10</td>\n",
       "      <td>AssertionError</td>\n",
       "      <td>Expectsdownloads99tobeequaltouploads100assert9...</td>\n",
       "      <td>123456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     category     arguments  \\\n",
       "pytest_failures_errors                                                        \n",
       "docs/multiple_objects_test.py    test_upload_multiple_objects  num=100-5-10   \n",
       "docs/multiple_objects_test.py    test_upload_multiple_objects  num=100-9-10   \n",
       "docs/multiple_objects_test.py  test_download_multiple_objects  num=100-3-10   \n",
       "docs/multiple_objects_test.py  test_download_multiple_objects  num=100-4-10   \n",
       "\n",
       "                                        error  \\\n",
       "pytest_failures_errors                          \n",
       "docs/multiple_objects_test.py  AssertionError   \n",
       "docs/multiple_objects_test.py  AssertionError   \n",
       "docs/multiple_objects_test.py  AssertionError   \n",
       "docs/multiple_objects_test.py  AssertionError   \n",
       "\n",
       "                                                                   error_details  \\\n",
       "pytest_failures_errors                                                             \n",
       "docs/multiple_objects_test.py  Expectsuploads100tobeequaltoobjectsinthebucket...   \n",
       "docs/multiple_objects_test.py  Expectsuploads100tobeequaltoobjectsinthebucket...   \n",
       "docs/multiple_objects_test.py  Expectsdownloads99tobeequaltouploads100assert9...   \n",
       "docs/multiple_objects_test.py  Expectsdownloads99tobeequaltouploads100assert9...   \n",
       "\n",
       "                               databaseId  \n",
       "pytest_failures_errors                     \n",
       "docs/multiple_objects_test.py      123456  \n",
       "docs/multiple_objects_test.py      123456  \n",
       "docs/multiple_objects_test.py      123456  \n",
       "docs/multiple_objects_test.py      123456  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "artifact = PytestArtifactLogExtractor(path = a[0])\n",
    "cat, categories, failures = artifact.log_to_df()\n",
    "\n",
    "display(cat)\n",
    "display(categories)\n",
    "display(failures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2924319023.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[406], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    Faz o teste -> gera graficos com tempo e taxa de falhas por tipo de teste\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Faz o teste -> gera graficos com tempo e taxa de falhas por tipo de teste\n",
    "\n",
    "Workflow -> Job -> Passos -> Resultados pytest \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define color mapping\n",
    "colors = {\n",
    "    'failure': 'firebrick',\n",
    "    'cancelled': 'darkgray',\n",
    "    'startup_failure': 'darkorange',\n",
    "    'success':  'darkgreen'\n",
    "}\n",
    "\n",
    "# Filter the DataFrame\n",
    "a = workflow.df[workflow.df['status'] == 'completed']\n",
    "\n",
    "# Get value counts of the 'conclusion' columnimport matplotlib.pyplot as plt\n",
    "\n",
    "# Define color mapping\n",
    "colors = {\n",
    "    'failure': 'firebrick',\n",
    "    'cancelled': 'darkgray',\n",
    "    'startup_failure':'darkorange',\n",
    "    'success':  'darkgreen'\n",
    "\n",
    "}\n",
    "\n",
    "# Filter the DataFrame\n",
    "a = workflow.df[workflow.df['status'] == 'completed']\n",
    "\n",
    "# Get value counts of the 'conclusion' column\n",
    "value_counts = a['conclusion'].value_counts()\n",
    "\n",
    "# Map colors to the categories in value_counts\n",
    "bar_colors = [colors[cat] for cat in value_counts.index]\n",
    "\n",
    "# Create a figure with two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))  # 1 row, 2 columns\n",
    "\n",
    "# Plot the bar chart on the first subplot\n",
    "value_counts.plot.bar(color=bar_colors, ax=ax1)\n",
    "ax1.set_xlabel('Conclusion')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('Bar Chart: Conclusion Counts')\n",
    "\n",
    "# Plot the pie chart on the second subplot\n",
    "value_counts.plot.pie(colors=bar_colors, autopct='%1.1f%%', ax=ax2)\n",
    "ax2.set_ylabel('')  # Remove the y-label for the pie chart\n",
    "ax2.set_title('Pie Chart: Conclusion Distribution')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n",
    "value_counts = a['conclusion'].value_counts()\n",
    "\n",
    "# Map colors to the categories in value_counts\n",
    "bar_colors = [colors[cat] for cat in value_counts.index]\n",
    "\n",
    "# Create a figure with two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))  # 1 row, 2 columns\n",
    "\n",
    "# Plot the bar chart on the first subplot\n",
    "value_counts.plot.bar(color=bar_colors, ax=ax1)\n",
    "ax1.set_xlabel('Conclusion')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('Bar Chart: Conclusion Counts')\n",
    "\n",
    "# Plot the pie chart on the second subplot\n",
    "value_counts.plot.pie(colors=bar_colors, autopct='%1.1f%%', ax=ax2)\n",
    "ax2.set_ylabel('')  # Remove the y-label for the pie chart\n",
    "ax2.set_title('Pie Chart: Conclusion Distribution')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jobs Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = ActionsJobs(repository=repo_path, workflow=workflow)\n",
    "ids = workflow.df['databaseId'].unique()\n",
    "all_job_dfs = [jobs.get_jobs(id)for id in ids]\n",
    "jobs_df = pd.concat(all_job_dfs)\n",
    "jobs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_failed_passed_jobs_bars(df):\n",
    "    unique_names = df.groupby(['Test', 'Conclusion']).size().unstack(fill_value=0)\n",
    "    test_to_number = {test: i + 1 for i, test in enumerate(df['Test'].unique())}\n",
    "\n",
    "    # Define colors for 'FAILED' and 'PASSED'\n",
    "    colors = {\n",
    "        'FAILED': 'firebrick',\n",
    "        'PASSED': 'darkgreen'\n",
    "    }\n",
    "\n",
    "    ax = unique_names.plot.bar(color=[colors['FAILED'], colors['PASSED']], figsize=(8, 4))\n",
    "\n",
    "    # Add labels and title\n",
    "    ax.set_xlabel('Test')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('FAILED vs PASSED by Test')\n",
    "\n",
    "    # Change the x-tick labels to their respective numbers\n",
    "    ax.set_xticklabels([test_to_number[test] for test in unique_names.index], rotation=0)\n",
    "\n",
    "    # Create a legend for the test numbers and names\n",
    "    test_legend = [f\"{num}. {test}\" for test, num in test_to_number.items()]\n",
    "    plt.figtext(1.05, 0.5, \"\\n\".join(test_legend), va='center', fontsize=10, wrap=True)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "#plot_failed_passed_jobs_bars(jobs_df[jobs_df['Conclusion'] == 'FAILED'])\n",
    "plot_failed_passed_jobs_bars(jobs_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
